#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬å–ç³»ç»Ÿè¯Šæ–­è„šæœ¬
ç”¨äºæ’æŸ¥"æ²¡æœ‰çˆ¬åˆ°ä»»ä½•æ•°æ®ï¼Œä¹Ÿæ²¡æœ‰æŠ¥é”™"çš„é—®é¢˜
"""

import asyncio
import json
import logging
import sys
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

import aiohttp
import pymongo
from pymongo import MongoClient
from bson import ObjectId

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('diagnose_crawl_system.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class CrawlSystemDiagnostic:
    """çˆ¬å–ç³»ç»Ÿè¯Šæ–­å™¨"""
    
    def __init__(self):
        self.config = self._load_config()
        self.mongo_client = None
        self.db = None
        self.issues = []
        self.recommendations = []
        
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open('config.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {
                "mongodb": {
                    "host": "localhost",
                    "port": 27017,
                    "database": "newshub"
                },
                "crawler_service": {
                    "host": "localhost",
                    "port": 8001
                },
                "backend_service": {
                    "host": "localhost",
                    "port": 8081
                }
            }
    
    async def run_diagnosis(self):
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹çˆ¬å–ç³»ç»Ÿè¯Šæ–­")
        logger.info("=" * 60)
        
        # 1. æ£€æŸ¥æ•°æ®åº“è¿æ¥
        await self._check_database_connection()
        
        # 2. æ£€æŸ¥æ•°æ®åº“é›†åˆçŠ¶æ€
        await self._check_database_collections()
        
        # 3. æ£€æŸ¥çˆ¬è™«æœåŠ¡çŠ¶æ€
        await self._check_crawler_service()
        
        # 4. æ£€æŸ¥åç«¯æœåŠ¡çŠ¶æ€
        await self._check_backend_service()
        
        # 5. æ£€æŸ¥ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
        await self._check_task_execution()
        
        # 6. æµ‹è¯•åˆ›å»ºçˆ¬å–ä»»åŠ¡
        await self._test_create_crawl_task()
        
        # 7. æ£€æŸ¥Workerçº¿ç¨‹çŠ¶æ€
        await self._check_worker_status()
        
        # 8. æ£€æŸ¥MCPæœåŠ¡çŠ¶æ€
        await self._check_mcp_service()
        
        # 9. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        self._generate_diagnosis_report()
        
        # æ¸…ç†èµ„æº
        if self.mongo_client:
            self.mongo_client.close()
    
    async def _check_database_connection(self):
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥"""
        logger.info("\n1. æ£€æŸ¥æ•°æ®åº“è¿æ¥...")
        try:
            mongo_config = self.config.get('mongodb', {})
            connection_string = f"mongodb://{mongo_config.get('host', 'localhost')}:{mongo_config.get('port', 27017)}"
            
            self.mongo_client = MongoClient(
                connection_string,
                serverSelectionTimeoutMS=5000,
                connectTimeoutMS=5000
            )
            
            # æµ‹è¯•è¿æ¥
            self.mongo_client.admin.command('ping')
            self.db = self.mongo_client[mongo_config.get('database', 'newshub')]
            
            logger.info("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
            logger.info(f"   è¿æ¥å­—ç¬¦ä¸²: {connection_string}")
            logger.info(f"   æ•°æ®åº“åç§°: {mongo_config.get('database', 'newshub')}")
            
        except Exception as e:
            error_msg = f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}"
            logger.error(error_msg)
            self.issues.append(error_msg)
            self.recommendations.append("æ£€æŸ¥MongoDBæœåŠ¡æ˜¯å¦å¯åŠ¨ï¼Œç«¯å£æ˜¯å¦æ­£ç¡®")
    
    async def _check_database_collections(self):
        """æ£€æŸ¥æ•°æ®åº“é›†åˆçŠ¶æ€"""
        logger.info("\n2. æ£€æŸ¥æ•°æ®åº“é›†åˆçŠ¶æ€...")
        
        if self.db is None:
            logger.error("âŒ æ•°æ®åº“æœªè¿æ¥ï¼Œè·³è¿‡é›†åˆæ£€æŸ¥")
            return
        
        try:
            collections = self.db.list_collection_names()
            logger.info(f"   ç°æœ‰é›†åˆ: {collections}")
            
            # æ£€æŸ¥å…³é”®é›†åˆ
            key_collections = ['crawl_tasks', 'continuous_tasks', 'platform_configs']
            for collection_name in key_collections:
                if collection_name in collections:
                    count = self.db[collection_name].count_documents({})
                    logger.info(f"âœ… {collection_name}: {count} æ¡è®°å½•")
                    
                    # æ˜¾ç¤ºæœ€è¿‘çš„è®°å½•
                    if count > 0:
                        recent_docs = list(self.db[collection_name].find().sort('_id', -1).limit(3))
                        for i, doc in enumerate(recent_docs):
                            doc_id = str(doc.get('_id', 'N/A'))
                            status = doc.get('status', 'N/A')
                            created_at = doc.get('created_at', 'N/A')
                            logger.info(f"     è®°å½•{i+1}: ID={doc_id[:8]}..., Status={status}, Created={created_at}")
                else:
                    logger.warning(f"âš ï¸  {collection_name}: é›†åˆä¸å­˜åœ¨")
                    self.issues.append(f"é›†åˆ {collection_name} ä¸å­˜åœ¨")
            
        except Exception as e:
            error_msg = f"âŒ æ£€æŸ¥æ•°æ®åº“é›†åˆå¤±è´¥: {e}"
            logger.error(error_msg)
            self.issues.append(error_msg)
    
    async def _check_crawler_service(self):
        """æ£€æŸ¥çˆ¬è™«æœåŠ¡çŠ¶æ€"""
        logger.info("\n3. æ£€æŸ¥çˆ¬è™«æœåŠ¡çŠ¶æ€...")
        
        try:
            crawler_config = self.config.get('crawler_service', {})
            base_url = f"http://{crawler_config.get('host', 'localhost')}:{crawler_config.get('port', 8001)}"
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                # æ£€æŸ¥å¥åº·çŠ¶æ€
                try:
                    async with session.get(f"{base_url}/health") as response:
                        if response.status == 200:
                            data = await response.json()
                            logger.info(f"âœ… çˆ¬è™«æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡")
                            logger.info(f"   æœåŠ¡åœ°å€: {base_url}")
                            logger.info(f"   å“åº”æ•°æ®: {data}")
                        else:
                            logger.warning(f"âš ï¸  çˆ¬è™«æœåŠ¡å¥åº·æ£€æŸ¥å¼‚å¸¸: HTTP {response.status}")
                except Exception as e:
                    logger.error(f"âŒ çˆ¬è™«æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
                    self.issues.append(f"çˆ¬è™«æœåŠ¡ä¸å¯è®¿é—®: {e}")
                    self.recommendations.append("æ£€æŸ¥çˆ¬è™«æœåŠ¡æ˜¯å¦å¯åŠ¨ï¼Œç«¯å£8001æ˜¯å¦å¯ç”¨")
                
                # æ£€æŸ¥APIç«¯ç‚¹
                endpoints = ["/docs", "/api/login-state/crawl", "/status"]
                for endpoint in endpoints:
                    try:
                        async with session.get(f"{base_url}{endpoint}") as response:
                            logger.info(f"   {endpoint}: HTTP {response.status}")
                    except Exception as e:
                        logger.warning(f"   {endpoint}: è®¿é—®å¤±è´¥ - {e}")
                        
        except Exception as e:
            error_msg = f"âŒ æ£€æŸ¥çˆ¬è™«æœåŠ¡å¤±è´¥: {e}"
            logger.error(error_msg)
            self.issues.append(error_msg)
    
    async def _check_backend_service(self):
        """æ£€æŸ¥åç«¯æœåŠ¡çŠ¶æ€"""
        logger.info("\n4. æ£€æŸ¥åç«¯æœåŠ¡çŠ¶æ€...")
        
        try:
            backend_config = self.config.get('backend_service', {})
            base_url = f"http://{backend_config.get('host', 'localhost')}:{backend_config.get('port', 8081)}"
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                # æ£€æŸ¥å¥åº·çŠ¶æ€
                try:
                    async with session.get(f"{base_url}/health") as response:
                        if response.status == 200:
                            logger.info(f"âœ… åç«¯æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡")
                            logger.info(f"   æœåŠ¡åœ°å€: {base_url}")
                        else:
                            logger.warning(f"âš ï¸  åç«¯æœåŠ¡å¥åº·æ£€æŸ¥å¼‚å¸¸: HTTP {response.status}")
                except Exception as e:
                    logger.error(f"âŒ åç«¯æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
                    self.issues.append(f"åç«¯æœåŠ¡ä¸å¯è®¿é—®: {e}")
                    self.recommendations.append("æ£€æŸ¥åç«¯æœåŠ¡æ˜¯å¦å¯åŠ¨ï¼Œç«¯å£8081æ˜¯å¦å¯ç”¨")
                
                # æ£€æŸ¥APIç«¯ç‚¹
                endpoints = ["/api/tasks", "/api/platforms"]
                for endpoint in endpoints:
                    try:
                        async with session.get(f"{base_url}{endpoint}") as response:
                            logger.info(f"   {endpoint}: HTTP {response.status}")
                            if response.status == 200:
                                data = await response.json()
                                if isinstance(data, list):
                                    logger.info(f"     è¿”å› {len(data)} æ¡è®°å½•")
                    except Exception as e:
                        logger.warning(f"   {endpoint}: è®¿é—®å¤±è´¥ - {e}")
                        
        except Exception as e:
            error_msg = f"âŒ æ£€æŸ¥åç«¯æœåŠ¡å¤±è´¥: {e}"
            logger.error(error_msg)
            self.issues.append(error_msg)
    
    async def _check_task_execution(self):
        """æ£€æŸ¥ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€"""
        logger.info("\n5. æ£€æŸ¥ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€...")
        
        if self.db is None:
            logger.error("âŒ æ•°æ®åº“æœªè¿æ¥ï¼Œè·³è¿‡ä»»åŠ¡æ‰§è¡Œæ£€æŸ¥")
            return
        
        try:
            # æ£€æŸ¥æœ€è¿‘24å°æ—¶çš„ä»»åŠ¡
            yesterday = datetime.now() - timedelta(days=1)
            
            # æ£€æŸ¥çˆ¬å–ä»»åŠ¡
            crawl_tasks = self.db.crawl_tasks
            recent_tasks = list(crawl_tasks.find({
                'created_at': {'$gte': yesterday.isoformat()}
            }).sort('created_at', -1).limit(10))
            
            logger.info(f"   æœ€è¿‘24å°æ—¶çˆ¬å–ä»»åŠ¡: {len(recent_tasks)} ä¸ª")
            
            if recent_tasks:
                status_count = {}
                for task in recent_tasks:
                    status = task.get('status', 'unknown')
                    status_count[status] = status_count.get(status, 0) + 1
                    
                    # æ˜¾ç¤ºä»»åŠ¡è¯¦æƒ…
                    task_id = str(task.get('_id', 'N/A'))[:8]
                    url = task.get('url', 'N/A')
                    created_at = task.get('created_at', 'N/A')
                    logger.info(f"     ä»»åŠ¡ {task_id}: {status} - {url} ({created_at})")
                
                logger.info(f"   ä»»åŠ¡çŠ¶æ€ç»Ÿè®¡: {status_count}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¡ä½çš„ä»»åŠ¡
                stuck_tasks = list(crawl_tasks.find({
                    'status': 'processing',
                    'created_at': {'$lt': (datetime.now() - timedelta(hours=1)).isoformat()}
                }))
                
                if stuck_tasks:
                    logger.warning(f"âš ï¸  å‘ç° {len(stuck_tasks)} ä¸ªå¯èƒ½å¡ä½çš„ä»»åŠ¡ï¼ˆå¤„ç†ä¸­è¶…è¿‡1å°æ—¶ï¼‰")
                    self.issues.append(f"æœ‰ {len(stuck_tasks)} ä¸ªä»»åŠ¡å¯èƒ½å¡ä½")
                    self.recommendations.append("æ£€æŸ¥Workerçº¿ç¨‹æ˜¯å¦æ­£å¸¸å·¥ä½œï¼Œè€ƒè™‘é‡å¯çˆ¬è™«æœåŠ¡")
            else:
                logger.warning("âš ï¸  æœ€è¿‘24å°æ—¶æ²¡æœ‰çˆ¬å–ä»»åŠ¡")
                self.issues.append("æœ€è¿‘24å°æ—¶æ²¡æœ‰åˆ›å»ºçˆ¬å–ä»»åŠ¡")
                self.recommendations.append("æ£€æŸ¥MCPæœåŠ¡æ˜¯å¦æ­£å¸¸è§¦å‘çˆ¬å–ä»»åŠ¡")
            
            # æ£€æŸ¥æŒç»­çˆ¬å–ä»»åŠ¡
            continuous_tasks = self.db.continuous_tasks
            active_continuous = list(continuous_tasks.find({'status': 'active'}))
            logger.info(f"   æ´»è·ƒçš„æŒç»­çˆ¬å–ä»»åŠ¡: {len(active_continuous)} ä¸ª")
            
            for task in active_continuous:
                task_id = str(task.get('_id', 'N/A'))[:8]
                platform = task.get('platform', 'N/A')
                last_crawl = task.get('last_crawl_time', 'N/A')
                logger.info(f"     æŒç»­ä»»åŠ¡ {task_id}: {platform} - æœ€åçˆ¬å–: {last_crawl}")
                
        except Exception as e:
            error_msg = f"âŒ æ£€æŸ¥ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€å¤±è´¥: {e}"
            logger.error(error_msg)
            self.issues.append(error_msg)
    
    async def _test_create_crawl_task(self):
        """æµ‹è¯•åˆ›å»ºçˆ¬å–ä»»åŠ¡"""
        logger.info("\n6. æµ‹è¯•åˆ›å»ºçˆ¬å–ä»»åŠ¡...")
        
        try:
            crawler_config = self.config.get('crawler_service', {})
            base_url = f"http://{crawler_config.get('host', 'localhost')}:{crawler_config.get('port', 8001)}"
            
            # æµ‹è¯•URL
            test_url = "https://weibo.com/u/1234567890"
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                payload = {
                    "session_id": "test-session",
                    "url": test_url,
                    "extract_content": True,
                    "extract_links": True,
                    "wait_time": 3
                }
                
                headers = {"X-User-ID": "test-user"}
                
                try:
                    async with session.post(f"{base_url}/api/login-state/crawl/create", json=payload, headers=headers) as response:
                        if response.status == 200:
                            data = await response.json()
                            task_id = data.get('task_id')
                            logger.info(f"âœ… æµ‹è¯•ä»»åŠ¡åˆ›å»ºæˆåŠŸ")
                            logger.info(f"   ä»»åŠ¡ID: {task_id}")
                            logger.info(f"   æµ‹è¯•URL: {test_url}")
                            
                            # ç­‰å¾…ä¸€æ®µæ—¶é—´åæ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                            await asyncio.sleep(5)
                            
                            async with session.get(f"{base_url}/api/login-state/crawl/{task_id}") as status_response:
                                if status_response.status == 200:
                                    status_data = await status_response.json()
                                    logger.info(f"   ä»»åŠ¡çŠ¶æ€: {status_data.get('status')}")
                                    logger.info(f"   ä»»åŠ¡è¯¦æƒ…: {status_data}")
                                else:
                                    logger.warning(f"âš ï¸  è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: HTTP {status_response.status}")
                        else:
                            error_text = await response.text()
                            logger.error(f"âŒ æµ‹è¯•ä»»åŠ¡åˆ›å»ºå¤±è´¥: HTTP {response.status}")
                            logger.error(f"   é”™è¯¯ä¿¡æ¯: {error_text}")
                            self.issues.append(f"æ— æ³•åˆ›å»ºæµ‹è¯•ä»»åŠ¡: HTTP {response.status}")
                            
                except Exception as e:
                    logger.error(f"âŒ æµ‹è¯•ä»»åŠ¡åˆ›å»ºè¯·æ±‚å¤±è´¥: {e}")
                    self.issues.append(f"æµ‹è¯•ä»»åŠ¡åˆ›å»ºè¯·æ±‚å¤±è´¥: {e}")
                    
        except Exception as e:
            error_msg = f"âŒ æµ‹è¯•åˆ›å»ºçˆ¬å–ä»»åŠ¡å¤±è´¥: {e}"
            logger.error(error_msg)
            self.issues.append(error_msg)
    
    async def _check_worker_status(self):
        """æ£€æŸ¥Workerçº¿ç¨‹çŠ¶æ€"""
        logger.info("\n7. æ£€æŸ¥Workerçº¿ç¨‹çŠ¶æ€...")
        
        try:
            crawler_config = self.config.get('crawler_service', {})
            base_url = f"http://{crawler_config.get('host', 'localhost')}:{crawler_config.get('port', 8001)}"
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                try:
                    async with session.get(f"{base_url}/status") as response:
                        if response.status == 200:
                            data = await response.json()
                            logger.info(f"âœ… çˆ¬è™«æœåŠ¡çŠ¶æ€è·å–æˆåŠŸ")
                            logger.info(f"   æ´»è·ƒWorkeræ•°é‡: {data.get('active_workers', 0)}")
                            logger.info(f"   é˜Ÿåˆ—ä¸­ä»»åŠ¡æ•°é‡: {data.get('queue_size', 0)}")
                            logger.info(f"   æœåŠ¡è¯¦æƒ…: {data}")
                            
                            if data.get('active_workers', 0) == 0:
                                logger.warning("âš ï¸  æ²¡æœ‰æ´»è·ƒçš„Workerçº¿ç¨‹")
                                self.issues.append("æ²¡æœ‰æ´»è·ƒçš„Workerçº¿ç¨‹")
                                self.recommendations.append("æ£€æŸ¥Workerç®¡ç†å™¨æ˜¯å¦æ­£å¸¸å¯åŠ¨")
                        else:
                            logger.warning(f"âš ï¸  è·å–çˆ¬è™«æœåŠ¡çŠ¶æ€å¤±è´¥: HTTP {response.status}")
                            self.issues.append(f"æ— æ³•è·å–çˆ¬è™«æœåŠ¡çŠ¶æ€: HTTP {response.status}")
                            
                except Exception as e:
                    logger.error(f"âŒ æ£€æŸ¥çˆ¬è™«æœåŠ¡çŠ¶æ€å¤±è´¥: {e}")
                    self.issues.append(f"çˆ¬è™«æœåŠ¡çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
                    self.recommendations.append("æ£€æŸ¥çˆ¬è™«æœåŠ¡çš„Workerç®¡ç†æ¨¡å—")
                    
        except Exception as e:
            error_msg = f"âŒ æ£€æŸ¥Workerçº¿ç¨‹çŠ¶æ€å¤±è´¥: {e}"
            logger.error(error_msg)
            self.issues.append(error_msg)
    
    async def _check_mcp_service(self):
        """æ£€æŸ¥MCPæœåŠ¡çŠ¶æ€"""
        logger.info("\n8. æ£€æŸ¥MCPæœåŠ¡çŠ¶æ€...")
        
        try:
            # æ£€æŸ¥MCPæœåŠ¡ç«¯å£ï¼ˆé€šå¸¸æ˜¯ä¸åŒçš„ç«¯å£ï¼‰
            mcp_ports = [8002, 8003]  # å‡è®¾çš„MCPæœåŠ¡ç«¯å£
            
            for port in mcp_ports:
                try:
                    async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5)) as session:
                        async with session.get(f"http://localhost:{port}/health") as response:
                            if response.status == 200:
                                logger.info(f"âœ… MCPæœåŠ¡ (ç«¯å£{port}) è¿è¡Œæ­£å¸¸")
                            else:
                                logger.warning(f"âš ï¸  MCPæœåŠ¡ (ç«¯å£{port}) å“åº”å¼‚å¸¸: HTTP {response.status}")
                except Exception as e:
                    logger.warning(f"âš ï¸  MCPæœåŠ¡ (ç«¯å£{port}) ä¸å¯è®¿é—®: {e}")
            
            # æ£€æŸ¥MCPç›¸å…³çš„æ•°æ®åº“è®°å½•
            if self.db is not None:
                # æ£€æŸ¥æ˜¯å¦æœ‰MCPè§¦å‘çš„ä»»åŠ¡
                mcp_tasks = list(self.db.crawl_tasks.find({
                    'source': 'mcp',
                    'created_at': {'$gte': (datetime.now() - timedelta(hours=24)).isoformat()}
                }).limit(5))
                
                logger.info(f"   æœ€è¿‘24å°æ—¶MCPè§¦å‘çš„ä»»åŠ¡: {len(mcp_tasks)} ä¸ª")
                
                if len(mcp_tasks) == 0:
                    logger.warning("âš ï¸  æœ€è¿‘24å°æ—¶æ²¡æœ‰MCPè§¦å‘çš„ä»»åŠ¡")
                    self.issues.append("MCPæœåŠ¡å¯èƒ½æ²¡æœ‰æ­£å¸¸è§¦å‘çˆ¬å–ä»»åŠ¡")
                    self.recommendations.append("æ£€æŸ¥MCPæµè§ˆå™¨é›†æˆå’Œé¡µé¢å¯¼èˆªç›‘å¬")
                    
        except Exception as e:
            error_msg = f"âŒ æ£€æŸ¥MCPæœåŠ¡çŠ¶æ€å¤±è´¥: {e}"
            logger.error(error_msg)
            self.issues.append(error_msg)
    
    def _generate_diagnosis_report(self):
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        logger.info("\n" + "=" * 60)
        logger.info("è¯Šæ–­æŠ¥å‘Š")
        logger.info("=" * 60)
        
        if not self.issues:
            logger.info("ğŸ‰ æ­å–œï¼æ²¡æœ‰å‘ç°æ˜æ˜¾é—®é¢˜")
            logger.info("\nå¯èƒ½çš„åŸå› ï¼š")
            logger.info("1. ç³»ç»Ÿåˆšå¯åŠ¨ï¼Œè¿˜æ²¡æœ‰è§¦å‘çˆ¬å–ä»»åŠ¡")
            logger.info("2. MCPæµè§ˆå™¨æ²¡æœ‰è®¿é—®ç›®æ ‡ç½‘ç«™")
            logger.info("3. çˆ¬å–ä»»åŠ¡æ­£åœ¨é˜Ÿåˆ—ä¸­ç­‰å¾…å¤„ç†")
            logger.info("\nå»ºè®®æ“ä½œï¼š")
            logger.info("1. ä½¿ç”¨MCPæµè§ˆå™¨è®¿é—®å¾®åšã€å°çº¢ä¹¦ç­‰ç›®æ ‡ç½‘ç«™")
            logger.info("2. ç‚¹å‡»å…·ä½“çš„å†…å®¹é¡µé¢è§¦å‘çˆ¬å–")
            logger.info("3. ç­‰å¾…å‡ åˆ†é’Ÿåæ£€æŸ¥æ•°æ®åº“ä¸­çš„ä»»åŠ¡è®°å½•")
        else:
            logger.error(f"\nâŒ å‘ç° {len(self.issues)} ä¸ªé—®é¢˜ï¼š")
            for i, issue in enumerate(self.issues, 1):
                logger.error(f"{i}. {issue}")
            
            logger.info(f"\nğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆï¼š")
            for i, recommendation in enumerate(self.recommendations, 1):
                logger.info(f"{i}. {recommendation}")
        
        logger.info("\n" + "=" * 60)
        logger.info("è¯Šæ–­å®Œæˆ")
        logger.info("=" * 60)
        
        # ä¿å­˜è¯Šæ–­ç»“æœåˆ°æ–‡ä»¶
        report = {
            "timestamp": datetime.now().isoformat(),
            "issues": self.issues,
            "recommendations": self.recommendations,
            "status": "healthy" if not self.issues else "issues_found"
        }
        
        try:
            with open('diagnosis_report.json', 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            logger.info("ğŸ“„ è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜åˆ° diagnosis_report.json")
        except Exception as e:
            logger.error(f"ä¿å­˜è¯Šæ–­æŠ¥å‘Šå¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    diagnostic = CrawlSystemDiagnostic()
    await diagnostic.run_diagnosis()

if __name__ == "__main__":
    asyncio.run(main())