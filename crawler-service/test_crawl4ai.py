#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
crawl4aiåŠŸèƒ½æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯crawl4aiæ˜¯å¦æ­£ç¡®å·¥ä½œ
"""

import asyncio
import logging
from crawl4ai import AsyncWebCrawler

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_basic_crawl():
    """æµ‹è¯•åŸºæœ¬çš„crawl4aiåŠŸèƒ½"""
    try:
        logger.info("å¼€å§‹æµ‹è¯•crawl4aiåŸºæœ¬åŠŸèƒ½...")
        
        # åˆ›å»ºå¼‚æ­¥çˆ¬è™«å®ä¾‹
        async with AsyncWebCrawler(
            verbose=True,
            headless=True,
            browser_type="chromium"
        ) as crawler:
            
            # æµ‹è¯•ç®€å•çš„ç½‘é¡µçˆ¬å–
            test_url = "https://httpbin.org/html"
            logger.info(f"æµ‹è¯•URL: {test_url}")
            
            result = await crawler.arun(
                url=test_url,
                word_count_threshold=10,
                bypass_cache=True
            )
            
            if result.success:
                logger.info("âœ… crawl4aiåŸºæœ¬åŠŸèƒ½æµ‹è¯•æˆåŠŸ")
                logger.info(f"é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A') if result.metadata else 'N/A'}")
                logger.info(f"å†…å®¹é•¿åº¦: {len(result.cleaned_html or '')} å­—ç¬¦")
                logger.info(f"Markdowné•¿åº¦: {len(result.markdown or '')} å­—ç¬¦")
                return True
            else:
                logger.error(f"âŒ crawl4aiçˆ¬å–å¤±è´¥: {result.error_message}")
                return False
                
    except Exception as e:
        logger.error(f"âŒ crawl4aiæµ‹è¯•å¼‚å¸¸: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return False

async def test_dynamic_content():
    """æµ‹è¯•åŠ¨æ€å†…å®¹çˆ¬å–"""
    try:
        logger.info("å¼€å§‹æµ‹è¯•crawl4aiåŠ¨æ€å†…å®¹åŠŸèƒ½...")
        
        async with AsyncWebCrawler(
            verbose=True,
            headless=True,
            browser_type="chromium",
            browser_args=[
                "--no-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu"
            ]
        ) as crawler:
            
            # æµ‹è¯•éœ€è¦JavaScriptæ¸²æŸ“çš„é¡µé¢
            test_url = "https://quotes.toscrape.com/js/"
            logger.info(f"æµ‹è¯•åŠ¨æ€å†…å®¹URL: {test_url}")
            
            result = await crawler.arun(
                url=test_url,
                word_count_threshold=10,
                bypass_cache=True,
                wait_for="networkidle",
                timeout=30000,
                delay_before_return_html=2
            )
            
            if result.success:
                content = result.cleaned_html or result.markdown or ''
                if 'quote' in content.lower() or len(content) > 500:
                    logger.info("âœ… crawl4aiåŠ¨æ€å†…å®¹æµ‹è¯•æˆåŠŸ")
                    logger.info(f"åŠ¨æ€å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    return True
                else:
                    logger.warning("âš ï¸ crawl4aiå¯èƒ½æœªæ­£ç¡®æ¸²æŸ“åŠ¨æ€å†…å®¹")
                    logger.info(f"è·å–çš„å†…å®¹: {content[:200]}...")
                    return False
            else:
                logger.error(f"âŒ crawl4aiåŠ¨æ€å†…å®¹çˆ¬å–å¤±è´¥: {result.error_message}")
                return False
                
    except Exception as e:
        logger.error(f"âŒ crawl4aiåŠ¨æ€å†…å®¹æµ‹è¯•å¼‚å¸¸: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return False

async def test_chinese_content():
    """æµ‹è¯•ä¸­æ–‡å†…å®¹çˆ¬å–"""
    try:
        logger.info("å¼€å§‹æµ‹è¯•crawl4aiä¸­æ–‡å†…å®¹åŠŸèƒ½...")
        
        async with AsyncWebCrawler(
            verbose=True,
            headless=True,
            browser_type="chromium",
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ) as crawler:
            
            # æµ‹è¯•ä¸­æ–‡ç½‘ç«™
            test_url = "https://www.baidu.com"
            logger.info(f"æµ‹è¯•ä¸­æ–‡å†…å®¹URL: {test_url}")
            
            result = await crawler.arun(
                url=test_url,
                word_count_threshold=5,
                bypass_cache=True
            )
            
            if result.success:
                content = result.cleaned_html or result.markdown or ''
                if 'ç™¾åº¦' in content or len(content) > 100:
                    logger.info("âœ… crawl4aiä¸­æ–‡å†…å®¹æµ‹è¯•æˆåŠŸ")
                    logger.info(f"ä¸­æ–‡å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    return True
                else:
                    logger.warning("âš ï¸ crawl4aiå¯èƒ½æœªæ­£ç¡®å¤„ç†ä¸­æ–‡å†…å®¹")
                    return False
            else:
                logger.error(f"âŒ crawl4aiä¸­æ–‡å†…å®¹çˆ¬å–å¤±è´¥: {result.error_message}")
                return False
                
    except Exception as e:
        logger.error(f"âŒ crawl4aiä¸­æ–‡å†…å®¹æµ‹è¯•å¼‚å¸¸: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("=" * 50)
    logger.info("å¼€å§‹crawl4aiåŠŸèƒ½æµ‹è¯•")
    logger.info("=" * 50)
    
    tests = [
        ("åŸºæœ¬åŠŸèƒ½æµ‹è¯•", test_basic_crawl),
        ("åŠ¨æ€å†…å®¹æµ‹è¯•", test_dynamic_content),
        ("ä¸­æ–‡å†…å®¹æµ‹è¯•", test_chinese_content)
    ]
    
    results = []
    for test_name, test_func in tests:
        logger.info(f"\n{'='*20} {test_name} {'='*20}")
        try:
            result = await test_func()
            results.append((test_name, result))
        except Exception as e:
            logger.error(f"æµ‹è¯• {test_name} å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            results.append((test_name, False))
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    logger.info("\n" + "=" * 50)
    logger.info("æµ‹è¯•ç»“æœæ±‡æ€»")
    logger.info("=" * 50)
    
    success_count = 0
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        logger.info(f"{test_name}: {status}")
        if success:
            success_count += 1
    
    logger.info(f"\næ€»ä½“ç»“æœ: {success_count}/{len(results)} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if success_count == len(results):
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œcrawl4aiåŠŸèƒ½æ­£å¸¸")
    elif success_count > 0:
        logger.warning("âš ï¸ éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œcrawl4aiå¯èƒ½å­˜åœ¨é—®é¢˜")
    else:
        logger.error("ğŸ’¥ æ‰€æœ‰æµ‹è¯•å¤±è´¥ï¼Œcrawl4aiå­˜åœ¨ä¸¥é‡é—®é¢˜")

if __name__ == "__main__":
    asyncio.run(main())