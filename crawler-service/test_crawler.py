#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ - éªŒè¯çˆ¬è™«æœåŠ¡åŠŸèƒ½
"""

import asyncio
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from crawl4ai import AsyncWebCrawler
from crawlers.platforms import CrawlerFactory, NewsCrawler

async def test_basic_crawl():
    """æµ‹è¯•åŸºç¡€çˆ¬å–åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºç¡€çˆ¬å–åŠŸèƒ½...")
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
        )
        
        print(f"âœ… æˆåŠŸçˆ¬å–é¡µé¢")
        print(f"ğŸ“„ æ ‡é¢˜: {result.metadata.get('title', 'N/A') if result.metadata else 'N/A'}")
        print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(result.markdown) if result.markdown else 0} å­—ç¬¦")
        print(f"ğŸ”— é“¾æ¥æ•°é‡: {len(result.links.get('internal', [])) if result.links else 0}")
        
        # æ˜¾ç¤ºéƒ¨åˆ†å†…å®¹
        if result.markdown:
            preview = result.markdown[:200] + "..." if len(result.markdown) > 200 else result.markdown
            print(f"ğŸ“– å†…å®¹é¢„è§ˆ:\n{preview}")

async def test_news_crawler():
    """æµ‹è¯•æ–°é—»çˆ¬è™«"""
    print("\nğŸ§ª æµ‹è¯•æ–°é—»çˆ¬è™«...")
    
    news_crawler = NewsCrawler()
    try:
        # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„æ–°é—»ç½‘ç«™è¿›è¡Œæµ‹è¯•
        articles = await news_crawler.crawl_news_articles("https://www.bbc.com/news", limit=3)
        
        print(f"âœ… æˆåŠŸçˆ¬å– {len(articles)} ç¯‡æ–‡ç« ")
        
        for i, article in enumerate(articles, 1):
            print(f"\nğŸ“° æ–‡ç«  {i}:")
            print(f"   æ ‡é¢˜: {article.title}")
            print(f"   å¹³å°: {article.platform}")
            print(f"   URL: {article.url}")
            print(f"   å†…å®¹é•¿åº¦: {len(article.content)} å­—ç¬¦")
            
    except Exception as e:
        print(f"âŒ æ–°é—»çˆ¬è™«æµ‹è¯•å¤±è´¥: {e}")
    finally:
        await news_crawler.cleanup()

async def test_platform_crawlers():
    """æµ‹è¯•å¹³å°çˆ¬è™«"""
    print("\nğŸ§ª æµ‹è¯•å¹³å°çˆ¬è™«...")
    
    platforms = CrawlerFactory.get_supported_platforms()
    print(f"ğŸ“± æ”¯æŒçš„å¹³å°: {', '.join(platforms)}")
    
    # æµ‹è¯•æ¯ä¸ªå¹³å°çš„çˆ¬è™«åˆå§‹åŒ–
    for platform in platforms:
        try:
            crawler = CrawlerFactory.get_crawler(platform)
            print(f"âœ… {platform} çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
            await crawler.cleanup()
        except Exception as e:
            print(f"âŒ {platform} çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")

async def test_example_from_docs():
    """æµ‹è¯•æ–‡æ¡£ä¸­çš„ç¤ºä¾‹ä»£ç """
    print("\nğŸ§ª æµ‹è¯•æ–‡æ¡£ç¤ºä¾‹...")
    
    try:
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(
                url="https://www.nbcnews.com/business",
            )
            print("âœ… æ–‡æ¡£ç¤ºä¾‹è¿è¡ŒæˆåŠŸ")
            print(f"ğŸ“Š ç»“æœç±»å‹: {type(result)}")
            print(f"ğŸ“„ æ˜¯å¦æœ‰å†…å®¹: {'æ˜¯' if result.markdown else 'å¦'}")
            
    except Exception as e:
        print(f"âŒ æ–‡æ¡£ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯• NewsHub Crawler Service")
    print("=" * 50)
    
    try:
        # æµ‹è¯•åŸºç¡€çˆ¬å–
        await test_basic_crawl()
        
        # æµ‹è¯•æ–°é—»çˆ¬è™«
        await test_news_crawler()
        
        # æµ‹è¯•å¹³å°çˆ¬è™«
        await test_platform_crawlers()
        
        # æµ‹è¯•æ–‡æ¡£ç¤ºä¾‹
        await test_example_from_docs()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())