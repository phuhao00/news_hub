# Manual Crawl Service for Login State Management
# Provides manual crawling functionality for logged-in websites

import asyncio
import logging
import json
import hashlib
import time
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Any, Set
from urllib.parse import urlparse, urljoin
import re
from enum import Enum

from playwright.async_api import Page, Browser, BrowserContext
from motor.motor_asyncio import AsyncIOMotorDatabase
from bson import ObjectId
from pydantic import BaseModel, HttpUrl
from bs4 import BeautifulSoup
import html2text

# Crawl4AI imports for intelligent extraction
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy

from .models import (
    ManualCrawlRequest,
    CrawlResult,
    CrawlTaskStatus,
    PlatformType,
    CrawlTaskDocument
)

# æŒç»­çˆ¬å–ç›¸å…³çš„æšä¸¾å’Œæ¨¡å‹
class ContinuousTaskStatus(str, Enum):
    """æŒç»­çˆ¬å–ä»»åŠ¡çŠ¶æ€"""
    RUNNING = "running"
    PAUSED = "paused"
    STOPPED = "stopped"
    ERROR = "error"

class ContinuousCrawlConfig(BaseModel):
    """æŒç»­çˆ¬å–é…ç½®"""
    crawl_interval_seconds: int = 30
    max_crawls: Optional[int] = None
    enable_deduplication: bool = True
    stop_on_no_changes: bool = False
    max_idle_time_seconds: int = 300  # 5åˆ†é’Ÿæ— å˜åŒ–ååœæ­¢
    content_change_threshold: float = 0.1  # å†…å®¹å˜åŒ–é˜ˆå€¼

class ContinuousCrawlTask(BaseModel):
    """æŒç»­çˆ¬å–ä»»åŠ¡"""
    task_id: str
    session_id: str
    user_id: str
    url: str
    platform: PlatformType
    status: ContinuousTaskStatus
    config: ContinuousCrawlConfig
    created_at: datetime
    last_crawl_at: Optional[datetime] = None
    next_crawl_at: Optional[datetime] = None
    crawl_count: int = 0
    content_hashes: List[str] = []
    last_content_hash: Optional[str] = None
    error_count: int = 0
    last_error: Optional[str] = None
from .session_manager import SessionManager
from .browser_manager import BrowserInstanceManager
from .cookie_store import CookieStore

logger = logging.getLogger(__name__)

def convert_objectid_to_str(document: dict) -> dict:
    """Convert MongoDB ObjectId fields to strings for JSON serialization"""
    if document is None:
        return None
    
    # Create a copy to avoid modifying the original
    converted = document.copy()
    
    # Convert _id field if present
    if '_id' in converted and isinstance(converted['_id'], ObjectId):
        converted['_id'] = str(converted['_id'])
    
    # Convert any other ObjectId fields recursively
    for key, value in converted.items():
        if isinstance(value, ObjectId):
            converted[key] = str(value)
        elif isinstance(value, dict):
            converted[key] = convert_objectid_to_str(value)
        elif isinstance(value, list):
            converted[key] = [convert_objectid_to_str(item) if isinstance(item, dict) else str(item) if isinstance(item, ObjectId) else item for item in value]
    
    return converted

class CrawlConfig(BaseModel):
    """Crawl configuration for different platforms"""
    platform: PlatformType
    extraction_schema: Dict[str, str]  # Schema for intelligent extraction
    wait_for: Optional[str] = "networkidle"  # Wait condition for page loading
    scroll_config: Optional[Dict[str, Any]] = None  # Scroll configuration
    custom_scripts: List[str] = []  # Custom JavaScript to execute
    timeout_ms: int = 30000  # Page load timeout
    word_count_threshold: int = 10  # Minimum word count for content
    
class ManualCrawlService:
    """Service for manual crawling of logged-in websites"""
    
    def __init__(
        self,
        db: AsyncIOMotorDatabase,
        session_manager: SessionManager,
        browser_manager: BrowserInstanceManager,
        cookie_store: CookieStore
    ):
        self.db = db
        self.session_manager = session_manager
        self.browser_manager = browser_manager
        self.cookie_store = cookie_store
        self.crawl_tasks = db.crawl_tasks
        
        # Platform-specific crawl configurations with intelligent extraction schemas
        self.platform_configs = {
            PlatformType.WEIBO: CrawlConfig(
                platform=PlatformType.WEIBO,
                extraction_schema={
                    "title": "The main title or headline of the Weibo post",
                    "content": "The main text content of the Weibo post",
                    "author": "The username or display name of the post author",
                    "publish_time": "The publication date and time of the post",
                    "images": "List of image URLs attached to the post",
                    "video": "Video URL if the post contains video content",
                    "likes": "Number of likes or reactions on the post",
                    "reposts": "Number of reposts or shares",
                    "comments": "Number of comments on the post"
                },
                wait_for="networkidle",
                scroll_config={"enabled": True, "max_scrolls": 3, "delay_ms": 1000},
                timeout_ms=15000,
                word_count_threshold=5
            ),
            PlatformType.XIAOHONGSHU: CrawlConfig(
                platform=PlatformType.XIAOHONGSHU,
                extraction_schema={
                    "title": "The title of the Xiaohongshu note or post",
                    "content": "The main description or content text of the note",
                    "author": "The username or nickname of the note author",
                    "publish_time": "The publication date of the note",
                    "images": "List of image URLs in the note",
                    "video": "Video URL if the note contains video",
                    "tags": "List of hashtags or tags associated with the note",
                    "likes": "Number of likes on the note",
                    "comments": "Number of comments on the note"
                },
                wait_for=None,
                scroll_config={"enabled": True, "max_scrolls": 2, "delay_ms": 1500},
                timeout_ms=10000,
                word_count_threshold=10,
                custom_scripts=[
                    "window.scrollTo(0, document.body.scrollHeight);",
                    "await new Promise(resolve => setTimeout(resolve, 2000));"
                ]
            ),
            PlatformType.DOUYIN: CrawlConfig(
                platform=PlatformType.DOUYIN,
                extraction_schema={
                    "title": "The title or caption of the Douyin video",
                    "content": "The description or caption text of the video",
                    "author": "The username of the video creator",
                    "publish_time": "The upload date and time of the video",
                    "video": "The main video URL",
                    "cover": "The video thumbnail or cover image URL",
                    "likes": "Number of likes on the video",
                    "comments": "Number of comments on the video",
                    "shares": "Number of shares of the video"
                },
                wait_for="networkidle",
                scroll_config={"enabled": False},
                timeout_ms=25000,
                word_count_threshold=5
            )
        }
        
        # Initialize Crawl4AI crawler
        self.crawler = None
        
        # ç™»å½•çŠ¶æ€æ£€æµ‹é…ç½®
        self.login_indicators = {
            PlatformType.WEIBO: {
                "logged_in_selectors": [".WB_miniblog", ".gn_name", ".Avatar_avatar", ".user-info"],
                "login_required_selectors": [".login", ".WB_login", ".login-panel"],
                "login_url": "https://weibo.com/login.php"
            },
            PlatformType.DOUYIN: {
                "logged_in_selectors": [".user-info", ".avatar", ".user-avatar"],
                "login_required_selectors": [".login-button", ".login-panel", ".login-mask"],
                "login_url": "https://www.douyin.com/passport/web/login/"
            },
            PlatformType.XIAOHONGSHU: {
                "logged_in_selectors": [".user-info", ".avatar-wrapper", ".user-avatar"],
                "login_required_selectors": [".login-container", ".sign-button", ".login-mask"],
                "login_url": "https://www.xiaohongshu.com/explore"
            }
        }
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.performance_config = {
            "max_content_length": 1024 * 1024,  # 1MB
            "dom_parse_timeout": 10,  # 10ç§’
            "crawl4ai_timeout": 30,  # 30ç§’
            "retry_delays": [1, 3, 5],  # é‡è¯•å»¶è¿Ÿ
            "max_retries": 3
        }
        
        # é”™è¯¯å¤„ç†é…ç½®
        self.error_patterns = {
            "network_timeout": ["timeout", "timed out", "connection timeout"],
            "anti_crawler": ["blocked", "captcha", "verification", "robot", "challenge"],
            "login_required": ["login required", "unauthorized", "please login", "sign in"],
            "page_not_found": ["404", "not found", "page not exist"],
            "rate_limit": ["rate limit", "too many requests", "è¯·æ±‚è¿‡äºé¢‘ç¹"]
        }
    
    async def create_crawl_task(
        self,
        request: ManualCrawlRequest,
        user_id: str
    ) -> str:
        """Create a new crawl task"""
        try:
            # Validate session
            session = await self.session_manager.validate_session(request.session_id)
            if not session or session.get('user_id') != user_id:
                raise ValueError("Invalid session")
            
            # Create task document
            task_doc = CrawlTaskDocument(
                task_id=f"crawl_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}_{user_id[:8]}",
                session_id=request.session_id,
                user_id=user_id,
                platform=session.get('platform'),  # ä»sessionä¸­è·å–platform
                url=str(request.url),
                status=CrawlTaskStatus.PENDING,
                request_data=request.dict(),  # ä¿å­˜è¯·æ±‚æ•°æ®
                created_at=datetime.now(timezone.utc)
            )
            
            # Insert task
            await self.crawl_tasks.insert_one(task_doc.dict())
            
            logger.info(f"Created crawl task {task_doc.task_id} for user {user_id}")
            return task_doc.task_id
            
        except Exception as e:
            logger.error(f"Failed to create crawl task: {e}")
            raise
    
    async def execute_crawl_task(self, task_id: str) -> CrawlResult:
        """Execute a crawl task"""
        try:
            # Get task
            task_doc = await self.crawl_tasks.find_one({"task_id": task_id})
            if not task_doc:
                raise ValueError(f"Task {task_id} not found")
            
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œçˆ¬å–ä»»åŠ¡ - ä»»åŠ¡ID: {task_id}, URL: {task_doc['url']}, å¹³å°: {task_doc['platform']}")
            
            # Update task status
            await self.crawl_tasks.update_one(
                {"task_id": task_id},
                {
                    "$set": {
                        "status": CrawlTaskStatus.RUNNING.value,
                        "started_at": datetime.now(timezone.utc),
                        "updated_at": datetime.now(timezone.utc)
                    }
                }
            )
            
            # Get browser instances for the session
            logger.info(f"Getting browser instances for session {task_doc['session_id']}")
            session_instances = await self.browser_manager.get_session_instances(
                task_doc["session_id"]
            )
            if not session_instances:
                logger.error(f"No browser instances found for session {task_doc['session_id']}")
                raise ValueError(f"No browser instance available for session {task_doc['session_id']}")
            
            logger.info(f"Found {len(session_instances)} browser instances for session {task_doc['session_id']}")
            
            # Select an active instance that's actually available in memory
            selected_instance = None
            instance_id = None
            
            for instance in session_instances:
                temp_instance_id = instance.get('instance_id')
                logger.debug(f"Checking instance {temp_instance_id}: active={instance.get('is_active', False)}")
                
                if instance.get("is_active", False):
                    # Verify the instance is actually available in memory
                    if temp_instance_id in self.browser_manager.browsers:
                        logger.info(f"Found available instance {temp_instance_id} in memory")
                        selected_instance = instance
                        instance_id = temp_instance_id
                        break
                    else:
                        logger.warning(f"Instance {temp_instance_id} is marked active in DB but not found in memory")
            
            if not selected_instance or not instance_id:
                # Log detailed information for debugging
                memory_instances = list(self.browser_manager.browsers.keys())
                db_instances = [inst.get('instance_id') for inst in session_instances if inst.get('is_active', False)]
                logger.error(f"No available browser instances found for session {task_doc['session_id']}")
                logger.error(f"DB active instances: {db_instances}")
                logger.error(f"Memory instances: {memory_instances}")
                raise ValueError(f"No available browser instance found for session {task_doc['session_id']}. DB has {len(db_instances)} active instances, memory has {len(memory_instances)} instances.")
            
            logger.info(f"Selected browser instance {instance_id} for crawl task {task_id}")
            
            # Get the browser instance using instance_id (this should now succeed)
            browser_instance = await self.browser_manager.get_browser_instance(instance_id)
            if not browser_instance:
                logger.error(f"Failed to get browser instance details for {instance_id} despite being in memory")
                raise ValueError(f"Failed to get browser instance details for {instance_id}")
            
            # Get the page from the browser manager's pages dictionary
            main_page_key = f"{instance_id}_main"
            page = self.browser_manager.pages.get(main_page_key)
            if not page:
                logger.error(f"No page available for browser instance {instance_id} (key: {main_page_key})")
                logger.debug(f"Available page keys: {list(self.browser_manager.pages.keys())}")
                raise ValueError(f"No page available in browser instance {instance_id}")
            
            logger.info(f"Successfully obtained page for crawl task {task_id} using instance {instance_id}")
            
            # Execute crawl
            result = await self._crawl_page(
                page,
                task_doc["url"],
                PlatformType(task_doc["platform"]),
                task_doc.get("config", {})
            )
            
            # Set the missing fields in the result
            result.task_id = task_id
            result.session_id = task_doc["session_id"]
            
            # Update task with result
            await self.crawl_tasks.update_one(
                {"task_id": task_id},
                {
                    "$set": {
                        "status": CrawlTaskStatus.COMPLETED.value,
                        "result": result.dict(),
                        "completed_at": datetime.now(timezone.utc),
                        "updated_at": datetime.now(timezone.utc)
                    }
                }
            )
            
            logger.info(f"Completed crawl task {task_id}")
            return result
            
        except Exception as e:
            # Update task with error
            await self.crawl_tasks.update_one(
                {"task_id": task_id},
                {
                    "$set": {
                        "status": CrawlTaskStatus.FAILED.value,
                        "error": str(e),
                        "failed_at": datetime.now(timezone.utc),
                        "updated_at": datetime.now(timezone.utc)
                    }
                }
            )
            
            logger.error(f"Failed to execute crawl task {task_id}: {e}")
            raise
    
    async def _crawl_page(
        self,
        page: Page,
        url: str,
        platform: PlatformType,
        custom_config: Optional[Dict[str, Any]] = None
    ) -> CrawlResult:
        """Crawl a single page using Playwright HTML content with Crawl4AI intelligent extraction"""
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ“„ å¼€å§‹çˆ¬å–é¡µé¢å†…å®¹ - URL: {url}, å¹³å°: {platform}")
            
            # æ£€æŸ¥ç™»å½•çŠ¶æ€
            login_status = await self._check_login_status(page, platform)
            if not login_status:
                logger.warning(f"æ£€æµ‹åˆ°æœªç™»å½•çŠ¶æ€ï¼Œå°è¯•è‡ªåŠ¨é‡æ–°ç™»å½•: {url}")
                await self._attempt_auto_login(page, platform)
            else:
                logger.info(f"âœ… ç™»å½•çŠ¶æ€éªŒè¯é€šè¿‡ - URL: {url}")
            
            # Get platform config
            config = self.platform_configs.get(platform)
            if not config:
                raise ValueError(f"Unsupported platform: {platform}")
            
            # Merge custom config
            if custom_config:
                config_dict = config.dict()
                config_dict.update(custom_config)
                config = CrawlConfig(**config_dict)
            
            # Get HTML content from the Playwright page
            logger.info(f"ğŸ“¥ è·å–é¡µé¢HTMLå†…å®¹ - URL: {url}")
            html_content = await page.content()
            logger.info(f"ğŸ“Š HTMLå†…å®¹è·å–å®Œæˆ - å¤§å°: {len(html_content)} bytes, URL: {url}")
            
            # æ£€æŸ¥å†…å®¹é•¿åº¦å¹¶æˆªæ–­
            if len(html_content) > self.performance_config["max_content_length"]:
                logger.warning(f"HTMLå†…å®¹è¿‡é•¿ ({len(html_content)} bytes)ï¼Œæˆªæ–­å¤„ç†")
                html_content = html_content[:self.performance_config["max_content_length"]]
            
            # åçˆ¬è™«æ£€æµ‹
            if await self._detect_anti_crawler(html_content, page):
                logger.warning(f"æ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶ï¼Œç­‰å¾…åé‡è¯•: {url}")
                await asyncio.sleep(5)  # ç­‰å¾…5ç§’
                html_content = await page.content()
            
            # Try Crawl4AI processing first, fallback to basic processing
            logger.info(f"ğŸ¤– å°è¯•ä½¿ç”¨Crawl4AIæ™ºèƒ½æå– - URL: {url}")
            try:
                crawl4ai_result = await asyncio.wait_for(
                    self._process_html_with_crawl4ai(html_content, url, platform),
                    timeout=self.performance_config["crawl4ai_timeout"]
                )
                if crawl4ai_result:
                    # Convert Crawl4AI result to CrawlResult format
                    result = self._convert_crawl4ai_result(crawl4ai_result, url)
                    processing_time = time.time() - start_time
                    logger.info(f"âœ… Crawl4AIæ™ºèƒ½æå–æˆåŠŸ - URL: {url}, è€—æ—¶: {processing_time:.2f}s, å†…å®¹é•¿åº¦: {len(result.content) if result.content else 0} å­—ç¬¦")
                    return result
            except asyncio.TimeoutError:
                logger.warning(f"â° Crawl4AIå¤„ç†è¶…æ—¶ï¼Œå›é€€åˆ°åŸºç¡€å¤„ç†: {url}")
            except Exception as e:
                error_type = self._classify_error(str(e))
                logger.warning(f"âŒ Crawl4AIå¤„ç†å¤±è´¥ ({error_type})ï¼Œå›é€€åˆ°åŸºç¡€å¤„ç†: {e}")
            
            # Fallback to basic HTML processing
            logger.info(f"ğŸ”§ ä½¿ç”¨åŸºç¡€HTMLå¤„ç†æ–¹æ³• - URL: {url}")
            result = await self._process_html_content(html_content, url, platform, config)
            processing_time = time.time() - start_time
            logger.info(f"âœ… åŸºç¡€å¤„ç†å®Œæˆ - URL: {url}, è€—æ—¶: {processing_time:.2f}s, å†…å®¹é•¿åº¦: {len(result.content) if result.content else 0} å­—ç¬¦")
            return result
            
        except Exception as e:
            error_type = self._classify_error(str(e))
            processing_time = time.time() - start_time
            logger.error(f"é¡µé¢çˆ¬å–å¤±è´¥ {url} ({error_type}) - è€—æ—¶: {processing_time:.2f}s: {e}")
            
            # æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦é‡è¯•
            if error_type in ["network_timeout", "rate_limit"]:
                await asyncio.sleep(3)  # ç­‰å¾…åå¯èƒ½é‡è¯•
            
            return CrawlResult(
                task_id="",
                session_id="",  # Will be set by caller
                url=url,
                status=CrawlTaskStatus.FAILED,
                title="",
                content=json.dumps({"error": str(e), "error_type": error_type}, ensure_ascii=False),
                links=[],
                images=[],
                metadata={
                    "error": str(e),
                    "error_type": error_type,
                    "processing_time": processing_time,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "extraction_method": "crawl4ai_llm_failed"
                },
                error_message=str(e),
                created_at=datetime.now(timezone.utc)
            )
    
    async def _process_html_content(
        self,
        html_content: str,
        url: str,
        platform: PlatformType,
        config: CrawlConfig
    ) -> CrawlResult:
        """Process HTML content directly without network requests"""
        start_time = time.time()  # æ·»åŠ start_timeå˜é‡å®šä¹‰
        try:
            logger.info(f"Processing HTML content for {url} (platform: {platform.value})")
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šé™åˆ¶HTMLå†…å®¹å¤§å°ï¼Œé¿å…å†…å­˜è¿‡åº¦ä½¿ç”¨
            max_html_size = self.performance_config.get("max_html_size", 5 * 1024 * 1024)  # 5MB
            if len(html_content) > max_html_size:
                logger.warning(f"HTML content too large ({len(html_content)} bytes), truncating to {max_html_size} bytes")
                html_content = html_content[:max_html_size]
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨lxmlè§£æå™¨ï¼ˆæ›´å¿«ï¼‰
            try:
                soup = BeautifulSoup(html_content, 'lxml')
            except:
                # å›é€€åˆ°html.parser
                soup = BeautifulSoup(html_content, 'html.parser')
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§æå–æ‰€æœ‰éœ€è¦çš„å…ƒç´ ï¼Œé¿å…å¤šæ¬¡éå†DOM
            title_tag = soup.find('title')
            title = title_tag.get_text().strip() if title_tag else ""
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in soup(["script", "style", "noscript", "iframe"]):
                element.decompose()
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šé™åˆ¶é“¾æ¥å’Œå›¾ç‰‡æå–æ•°é‡
            max_links = self.performance_config.get("max_links_extract", 50)
            max_images = self.performance_config.get("max_images_extract", 30)
            
            # ä¸€æ¬¡æ€§æå–é“¾æ¥å’Œå›¾ç‰‡
            link_elements = soup.find_all('a', href=True, limit=max_links)
            img_elements = soup.find_all('img', src=True, limit=max_images)
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ï¼Œæ›´é«˜æ•ˆ
            links = [{
                'url': link['href'],
                'text': link.get_text().strip()[:100],  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                'title': link.get('title', '')[:100]
            } for link in link_elements]
            
            images = [{
                'url': img['src'],
                'alt': img.get('alt', '')[:100],
                'title': img.get('title', '')[:100]
            } for img in img_elements]
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šå»¶è¿Ÿæ–‡æœ¬å†…å®¹æå–ï¼Œåªåœ¨éœ€è¦æ—¶è¿›è¡Œ
            text_content = None
            content = None
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šå»¶è¿Ÿåˆå§‹åŒ–æ–‡æœ¬å†…å®¹æå–å‡½æ•°
            def get_text_content():
                nonlocal text_content, content
                if text_content is None:
                    text_content = soup.get_text()
                    lines = (line.strip() for line in text_content.splitlines())
                    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                    content = '\n'.join(chunk for chunk in chunks if chunk)
                return text_content, content
            
            # Try to use Crawl4AI for intelligent extraction if available
            extracted_data = {}
            crawl4ai_timeout = self.performance_config.get("crawl4ai_timeout", 30)  # 30ç§’è¶…æ—¶
            
            try:
                # æ€§èƒ½ä¼˜åŒ–ï¼šæ·»åŠ è¶…æ—¶æ§åˆ¶
                async with asyncio.timeout(crawl4ai_timeout):
                    if not self.crawler:
                        browser_config = BrowserConfig(
                            headless=True,
                            browser_type="chromium"
                        )
                        self.crawler = AsyncWebCrawler(config=browser_config)
                        logger.info("Crawl4AI crawler initialized for HTML processing")
                    
                    # Create extraction strategy with platform-specific schema
                    extraction_strategy = LLMExtractionStrategy(
                        llm_config=LLMConfig(provider="openai"),
                        schema=config.extraction_schema,
                        extraction_type="schema",
                        instruction=f"Extract structured data from this {platform.value} page. Focus on the main content, author information, engagement metrics, and media URLs. Ensure all extracted text is clean and properly formatted."
                    )
                    
                    # Configure crawler run settings for HTML processing
                    run_config = CrawlerRunConfig(
                        word_count_threshold=config.word_count_threshold,
                        extraction_strategy=extraction_strategy,
                        bypass_cache=True  # Don't use cache for HTML processing
                    )
                    
                    # Process HTML with Crawl4AI (no network request)
                    result = await self.crawler.arun(
                        url=url,
                        html=html_content,
                        config=run_config
                    )
                    
                    if result.success and result.extracted_content:
                        try:
                            if isinstance(result.extracted_content, str):
                                extracted_data = json.loads(result.extracted_content)
                            else:
                                extracted_data = result.extracted_content
                            
                            # Handle case where extracted_data is a list
                            if isinstance(extracted_data, list):
                                if extracted_data:
                                    extracted_data = {"items": extracted_data}
                                else:
                                    extracted_data = {}
                            
                            logger.info(f"Crawl4AI extraction successful: {list(extracted_data.keys()) if isinstance(extracted_data, dict) else 'list format'}")
                        except (json.JSONDecodeError, TypeError) as e:
                            logger.warning(f"Failed to parse Crawl4AI extracted content: {e}")
                            extracted_data = {}
                    else:
                        logger.warning("Crawl4AI extraction failed, using fallback")
                        
            except asyncio.TimeoutError:
                logger.warning(f"Crawl4AI processing timeout after {crawl4ai_timeout}s, using fallback extraction")
            except Exception as e:
                logger.warning(f"Crawl4AI processing failed: {e}, using fallback extraction")
            
            # Fallback extraction if Crawl4AI failed
            if not extracted_data:
                # æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨å»¶è¿ŸåŠ è½½çš„æ–‡æœ¬å†…å®¹
                text_content, content = get_text_content()
                
                # Convert links and images to URL strings using unified method
                fallback_links = self._extract_urls_from_result(links[:10])
                fallback_images = self._extract_urls_from_result(images[:10])
                
                extracted_data = {
                    "title": title,
                    "content": content[:2000] if content else "",  # Limit content length
                    "links": fallback_links,  # Convert to URL strings
                    "images": fallback_images,  # Convert to URL strings
                    "text_length": len(content) if content else 0,
                    "extraction_method": "fallback_beautifulsoup"
                }
                logger.info(f"Using fallback BeautifulSoup extraction - Title: {title[:50]}{'...' if len(title) > 50 else ''}, Content length: {len(content) if content else 0}, Links: {len(fallback_links)}, Images: {len(fallback_images)}")
            
            # æ€§èƒ½ç›‘æ§ï¼šè®°å½•å¤„ç†æ—¶é—´
            processing_end_time = time.time()
            processing_time = processing_end_time - start_time
            
            # è¯¦ç»†æ—¥å¿—è®°å½•
            logger.info(f"HTML processing completed for {platform.value} - URL: {url[:100]}{'...' if len(url) > 100 else ''}")
            logger.info(f"Processing metrics - Time: {processing_time:.2f}s, HTML size: {len(html_content)} bytes, Title: {title[:50]}{'...' if len(title) > 50 else ''}")
            logger.info(f"Extraction results - Links: {len(links)}, Images: {len(images)}, Content length: {len(content) if content else 0} chars")
            
            # Prepare metadata with performance metrics
            metadata = {
                "url": url,
                "platform": platform.value,
                "crawl_timestamp": datetime.now(timezone.utc).isoformat(),
                "word_count": len(content.split()) if content else 0,
                "success": True,
                "page_title": title,
                "content_length": len(html_content),
                "link_count": len(links),
                "image_count": len(images),
                "extraction_method": "html_processing_with_playwright",
                "processing_time_seconds": round(processing_time, 2),
                "html_size_bytes": len(html_content),
                "performance_metrics": {
                    "dom_parsing_optimized": True,
                    "lazy_text_extraction": True,
                    "crawl4ai_timeout": crawl4ai_timeout,
                    "content_truncated": len(html_content) > self.performance_config.get("max_html_size", 1000000)
                }
            }
            
            # Add meta tag information
            for meta in soup.find_all('meta'):
                name = meta.get('name') or meta.get('property')
                content_attr = meta.get('content')
                if name and content_attr:
                    metadata[f'meta_{name}'] = content_attr
            
            # Convert content to JSON string for storage
            content_json = json.dumps(extracted_data, ensure_ascii=False, default=str)
            
            # Convert links and images to string lists for CrawlResult model using unified method
            result_links = self._extract_urls_from_result(extracted_data.get("links", []))
            if not result_links:  # Fallback to BeautifulSoup extracted links
                result_links = self._extract_urls_from_result(links)
            
            result_images = self._extract_urls_from_result(extracted_data.get("images", []))
            if not result_images:  # Fallback to BeautifulSoup extracted images
                result_images = self._extract_urls_from_result(images)
            
            # Also extract from media_urls if available
            if "media_urls" in extracted_data:
                media_links = self._extract_urls_from_result(extracted_data["media_urls"])
                result_images.extend([url for url in media_links if any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.mp4', '.mov'])])
                result_links.extend([url for url in media_links if not any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.mp4', '.mov'])])
            
            # Remove duplicates and limit count
            result_links = list(dict.fromkeys(result_links))[:20]  # Remove duplicates and limit
            result_images = list(dict.fromkeys(result_images))[:20]  # Remove duplicates and limit
            
            # è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°
            quality_score = self._calculate_data_quality_score(extracted_data)
            metadata["data_quality_score"] = quality_score
            
            # å¦‚æœè´¨é‡åˆ†æ•°è¾ƒä½ï¼Œå°è¯•å¢å¼ºfallback
            if quality_score < 0.5:
                logger.info(f"Low quality extraction (score: {quality_score}), trying enhanced fallback")
                enhanced_result = self._enhanced_fallback_extraction(html_content)
                
                if enhanced_result and (enhanced_result.get("title") or enhanced_result.get("content")):
                    # åˆå¹¶ç»“æœï¼Œä¼˜å…ˆä½¿ç”¨è´¨é‡æ›´å¥½çš„æ•°æ®
                    merged_result = self._merge_extraction_results(extracted_data, enhanced_result)
                    content_json = json.dumps(merged_result, ensure_ascii=False, default=str)
                    
                    # æ›´æ–°é“¾æ¥å’Œå›¾ç‰‡
                    result_links = self._extract_urls_from_result(merged_result.get("links", []))
                    result_images = self._extract_urls_from_result(merged_result.get("images", []))
                    
                    # æ›´æ–°è´¨é‡åˆ†æ•°
                    quality_score = self._calculate_data_quality_score(merged_result)
                    metadata["data_quality_score"] = quality_score
                    metadata["extraction_method"] = "enhanced_fallback_merged"
            
            return CrawlResult(
                task_id="",  # Will be set by caller
                session_id="",  # Will be set by caller
                url=url,
                status=CrawlTaskStatus.COMPLETED,
                title=extracted_data.get("title", title),
                content=content_json,
                links=result_links,
                images=result_images,
                metadata=metadata,
                created_at=datetime.now(timezone.utc)
            )
            
        except Exception as e:
            logger.error(f"Failed to process HTML content for {url}: {e}")
            return CrawlResult(
                task_id="",
                session_id="",
                url=url,
                status=CrawlTaskStatus.FAILED,
                title="",
                content=json.dumps({"error": str(e)}, ensure_ascii=False),
                links=[],
                images=[],
                metadata={
                    "error": str(e),
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "extraction_method": "crawl4ai_llm_with_playwright_failed"
                },
                error_message=str(e),
                created_at=datetime.now(timezone.utc)
            )
    
    async def _process_html_with_crawl4ai(self, html_content: str, url: str, platform: PlatformType) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨Crawl4AIç›´æ¥å¤„ç†HTMLå†…å®¹ï¼Œé¿å…é‡å¤ç½‘ç»œè¯·æ±‚"""
        try:
            # åˆ›å»ºCrawl4AIå®¢æˆ·ç«¯
            async with AsyncWebCrawler(
                 headless=True,
                 browser_type="chromium",
                 verbose=False
             ) as crawler:
                # æ ¹æ®å¹³å°å®šåˆ¶æå–ç­–ç•¥
                extraction_strategy = self._get_extraction_strategy(platform)
                
                # ç›´æ¥å¤„ç†HTMLå†…å®¹ï¼Œé¿å…é‡å¤è¯·æ±‚
                # ä½¿ç”¨Crawl4AIçš„HTMLå¤„ç†èƒ½åŠ›è€Œä¸æ˜¯ç½‘ç»œçˆ¬å–
                from crawl4ai.content_filter import ContentFilter
                from crawl4ai.markdown_generation import MarkdownGenerator
                from crawl4ai.extraction_strategy import ExtractionStrategy
                
                # åˆ›å»ºå†…å®¹è¿‡æ»¤å™¨å’ŒMarkdownç”Ÿæˆå™¨
                content_filter = ContentFilter()
                markdown_generator = MarkdownGenerator()
                
                # æ¸…ç†HTMLå†…å®¹
                cleaned_html = content_filter.filter_content(html_content)
                
                # ç”ŸæˆMarkdown
                markdown_content = markdown_generator.generate_markdown(
                    cleaned_html,
                    base_url=url
                )
                
                # åº”ç”¨æå–ç­–ç•¥
                extracted_content = {}
                if extraction_strategy and hasattr(extraction_strategy, 'extract'):
                    try:
                        extraction_result = await extraction_strategy.extract(
                            url=url,
                            html=cleaned_html,
                            markdown=markdown_content
                        )
                        
                        if isinstance(extraction_result, str):
                            extracted_content = json.loads(extraction_result)
                        else:
                            extracted_content = extraction_result
                    except Exception as e:
                        logger.warning(f"Extraction strategy failed for {url}: {e}")
                        # ä½¿ç”¨åŸºç¡€æå–ä½œä¸ºfallback
                        extracted_content = self._basic_content_extraction(cleaned_html, markdown_content)
                else:
                    # ä½¿ç”¨åŸºç¡€æå–
                    extracted_content = self._basic_content_extraction(cleaned_html, markdown_content)
                
                return {
                    "title": extracted_content.get("title", ""),
                    "content": extracted_content.get("content", markdown_content or ""),
                    "author": extracted_content.get("author", ""),
                    "publish_time": extracted_content.get("publish_time", ""),
                    "tags": extracted_content.get("tags", []),
                    "summary": extracted_content.get("summary", ""),
                    "links": extracted_content.get("links", []),
                    "images": extracted_content.get("images", []),
                    "metadata": {
                        "extraction_method": "crawl4ai_html_direct",
                        "platform": platform.value,
                        "content_length": len(markdown_content or ""),
                        "success": True
                    }
                }
                    
        except Exception as e:
            logger.error(f"Error in Crawl4AI HTML processing for {url}: {e}")
            return None
    
    def _get_extraction_strategy(self, platform: PlatformType):
         """è·å–å¹³å°ç‰¹å®šçš„æå–ç­–ç•¥"""
         try:
             # æ ¹æ®å¹³å°åˆ›å»ºä¼˜åŒ–çš„æå–æŒ‡ä»¤
             platform_instructions = {
                 PlatformType.WEIBO: """
                 ä»å¾®åšé¡µé¢æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
                 1. æ ‡é¢˜ï¼šå¾®åšæ­£æ–‡å†…å®¹çš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜
                 2. å†…å®¹ï¼šå®Œæ•´çš„å¾®åšæ­£æ–‡ï¼ŒåŒ…æ‹¬è¯é¢˜æ ‡ç­¾å’Œ@ç”¨æˆ·
                 3. ä½œè€…ï¼šå‘å¸ƒè€…çš„ç”¨æˆ·åå’Œè®¤è¯ä¿¡æ¯
                 4. å‘å¸ƒæ—¶é—´ï¼šå‡†ç¡®çš„å‘å¸ƒæ—¶é—´æˆ³
                 5. äº’åŠ¨æ•°æ®ï¼šç‚¹èµæ•°ã€è½¬å‘æ•°ã€è¯„è®ºæ•°
                 6. åª’ä½“æ–‡ä»¶ï¼šå›¾ç‰‡ã€è§†é¢‘çš„å®Œæ•´URL
                 7. è¯é¢˜æ ‡ç­¾ï¼š#è¯é¢˜#æ ¼å¼çš„æ ‡ç­¾
                 8. ä½ç½®ä¿¡æ¯ï¼šå¦‚æœæœ‰åœ°ç†ä½ç½®æ ‡è®°
                 """,
                 PlatformType.DOUYIN: """
                 ä»æŠ–éŸ³é¡µé¢æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
                 1. æ ‡é¢˜ï¼šè§†é¢‘æ ‡é¢˜æˆ–æè¿°æ–‡å­—
                 2. å†…å®¹ï¼šè§†é¢‘æè¿°ã€è¯é¢˜æ ‡ç­¾ã€èƒŒæ™¯éŸ³ä¹ä¿¡æ¯
                 3. ä½œè€…ï¼šåˆ›ä½œè€…ç”¨æˆ·åã€ç²‰ä¸æ•°ã€è®¤è¯çŠ¶æ€
                 4. å‘å¸ƒæ—¶é—´ï¼šè§†é¢‘å‘å¸ƒçš„æ—¶é—´
                 5. äº’åŠ¨æ•°æ®ï¼šç‚¹èµæ•°ã€è¯„è®ºæ•°ã€åˆ†äº«æ•°ã€æ’­æ”¾é‡
                 6. åª’ä½“æ–‡ä»¶ï¼šè§†é¢‘å°é¢å›¾ã€è§†é¢‘URL
                 7. éŸ³ä¹ä¿¡æ¯ï¼šèƒŒæ™¯éŸ³ä¹åç§°å’Œä½œè€…
                 8. æŒ‘æˆ˜è¯é¢˜ï¼šå‚ä¸çš„æŒ‘æˆ˜æˆ–è¯é¢˜
                 """,
                 PlatformType.XIAOHONGSHU: """
                 ä»å°çº¢ä¹¦é¡µé¢æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
                 1. æ ‡é¢˜ï¼šç¬”è®°æ ‡é¢˜
                 2. å†…å®¹ï¼šç¬”è®°æ­£æ–‡å†…å®¹ï¼ŒåŒ…æ‹¬å•†å“é“¾æ¥å’Œæ ‡ç­¾
                 3. ä½œè€…ï¼šåšä¸»ç”¨æˆ·åã€ç²‰ä¸æ•°ã€ç­‰çº§ä¿¡æ¯
                 4. å‘å¸ƒæ—¶é—´ï¼šç¬”è®°å‘å¸ƒæ—¶é—´
                 5. äº’åŠ¨æ•°æ®ï¼šç‚¹èµæ•°ã€æ”¶è—æ•°ã€è¯„è®ºæ•°
                 6. åª’ä½“æ–‡ä»¶ï¼šå›¾ç‰‡URLåˆ—è¡¨ï¼Œè§†é¢‘URL
                 7. å•†å“ä¿¡æ¯ï¼šå…³è”çš„å•†å“é“¾æ¥å’Œä»·æ ¼
                 8. æ ‡ç­¾ï¼šç¬”è®°ç›¸å…³çš„æ ‡ç­¾å’Œåˆ†ç±»
                 """,
                 PlatformType.BILIBILI: """
                 ä»Bç«™é¡µé¢æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
                 1. æ ‡é¢˜ï¼šè§†é¢‘æ ‡é¢˜
                 2. å†…å®¹ï¼šè§†é¢‘ç®€ä»‹ã€åˆ†Pä¿¡æ¯ã€ç›¸å…³é“¾æ¥
                 3. ä½œè€…ï¼šUPä¸»ç”¨æˆ·åã€ç²‰ä¸æ•°ã€ç­‰çº§
                 4. å‘å¸ƒæ—¶é—´ï¼šè§†é¢‘å‘å¸ƒæ—¶é—´
                 5. äº’åŠ¨æ•°æ®ï¼šæ’­æ”¾é‡ã€ç‚¹èµæ•°ã€æŠ•å¸æ•°ã€æ”¶è—æ•°ã€åˆ†äº«æ•°
                 6. åª’ä½“æ–‡ä»¶ï¼šè§†é¢‘å°é¢ã€è§†é¢‘URL
                 7. åˆ†åŒºä¿¡æ¯ï¼šè§†é¢‘æ‰€å±åˆ†åŒºå’Œæ ‡ç­¾
                 8. å¼¹å¹•æ•°ï¼šå¼¹å¹•æ€»æ•°
                 """
             }
             
             instruction = platform_instructions.get(platform, 
                 f"Extract structured data from this {platform.value} page. Focus on the main content, author information, engagement metrics, and media URLs.")
             
             # åˆ›å»ºLLMæå–ç­–ç•¥
             try:
                 from crawl4ai.extraction_strategy import LLMExtractionStrategy
                 from crawl4ai import LLMConfig
                 
                 return LLMExtractionStrategy(
                     llm_config=LLMConfig(provider="openai"),
                     extraction_type="schema",
                     instruction=instruction,
                     schema={
                         "type": "object",
                         "properties": {
                             "title": {"type": "string", "description": "å†…å®¹æ ‡é¢˜"},
                             "content": {"type": "string", "description": "ä¸»è¦å†…å®¹"},
                             "author": {"type": "string", "description": "ä½œè€…ä¿¡æ¯"},
                             "publish_time": {"type": "string", "description": "å‘å¸ƒæ—¶é—´"},
                             "tags": {"type": "array", "items": {"type": "string"}, "description": "æ ‡ç­¾åˆ—è¡¨"},
                             "summary": {"type": "string", "description": "å†…å®¹æ‘˜è¦"},
                             "engagement": {
                                 "type": "object",
                                 "properties": {
                                     "likes": {"type": "integer"},
                                     "shares": {"type": "integer"},
                                     "comments": {"type": "integer"},
                                     "views": {"type": "integer"}
                                 }
                             },
                             "media_urls": {
                                 "type": "object",
                                 "properties": {
                                     "images": {"type": "array", "items": {"type": "string"}},
                                     "videos": {"type": "array", "items": {"type": "string"}}
                                 }
                             }
                         },
                         "required": ["title", "content", "author"]
                     }
                 )
             except ImportError:
                 logger.warning("Crawl4AI LLM extraction not available, using basic extraction")
                 return None
                 
         except Exception as e:
             logger.warning(f"Failed to create extraction strategy for {platform}: {e}")
             return None
    
    def _basic_content_extraction(self, html_content: str, markdown_content: str) -> Dict[str, Any]:
        """åŸºç¡€å†…å®¹æå–ä½œä¸ºfallback"""
        try:
            from bs4 import BeautifulSoup
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–æ ‡é¢˜
            title = ""
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text().strip()
            
            # æå–é“¾æ¥
            links = []
            for link in soup.find_all('a', href=True):
                links.append({
                    'url': link['href'],
                    'text': link.get_text().strip()
                })
            
            # æå–å›¾ç‰‡
            images = []
            for img in soup.find_all('img', src=True):
                images.append({
                    'url': img['src'],
                    'alt': img.get('alt', '')
                })
            
            return {
                "title": title,
                "content": markdown_content or "",
                "links": links[:10],  # é™åˆ¶æ•°é‡
                "images": images[:10],  # é™åˆ¶æ•°é‡
                "extraction_method": "basic_fallback"
            }
        except Exception as e:
            logger.error(f"Basic content extraction failed: {e}")
            return {"content": markdown_content or "", "extraction_method": "minimal_fallback"}
    
    def _enhanced_fallback_extraction(self, html_content: str) -> Dict[str, Any]:
        """å¢å¼ºçš„fallbackæå–æœºåˆ¶"""
        try:
            from bs4 import BeautifulSoup
            import re
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æ™ºèƒ½æ ‡é¢˜æå–
            title = self._extract_smart_title(soup)
            
            # æ™ºèƒ½å†…å®¹æå–
            content = self._extract_smart_content(soup)
            
            # æ™ºèƒ½ä½œè€…æå–
            author = self._extract_smart_author(soup)
            
            # æå–æ—¶é—´ä¿¡æ¯
            publish_time = self._extract_publish_time(soup)
            
            # æå–æ ‡ç­¾
            tags = self._extract_tags(soup)
            
            # æå–äº’åŠ¨æ•°æ®
            engagement = self._extract_engagement_data(soup)
            
            # æå–åª’ä½“URL
            media_urls = self._extract_media_urls(soup)
            
            return {
                'title': title,
                'content': content,
                'author': author,
                'publish_time': publish_time,
                'tags': tags,
                'summary': content[:200] + '...' if len(content) > 200 else content,
                'engagement': engagement,
                'media_urls': media_urls
            }
            
        except Exception as e:
            logger.error(f"Enhanced fallback extraction failed: {e}")
            return {
                'title': '',
                'content': '',
                'author': '',
                'publish_time': '',
                'tags': [],
                'summary': '',
                'engagement': {},
                'media_urls': {'images': [], 'videos': []}
            }
    
    def _extract_smart_title(self, soup) -> str:
        """æ™ºèƒ½æ ‡é¢˜æå–"""
        # ä¼˜å…ˆçº§é¡ºåºçš„æ ‡é¢˜é€‰æ‹©å™¨
        title_selectors = [
            'h1',
            '.title',
            '[data-title]',
            '.post-title',
            '.article-title',
            'title'
        ]
        
        for selector in title_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                if text and len(text) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„æ ‡é¢˜
                    return text
        
        return ''
    
    def _extract_smart_content(self, soup) -> str:
        """æ™ºèƒ½å†…å®¹æå–"""
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()
        
        # å†…å®¹é€‰æ‹©å™¨ä¼˜å…ˆçº§
        content_selectors = [
            '.content',
            '.post-content',
            '.article-content',
            'main',
            '.main-content',
            'article',
            '.text'
        ]
        
        for selector in content_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                if text and len(text) > 50:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                    return text
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå†…å®¹åŒºåŸŸï¼Œæå–bodyä¸­çš„æ–‡æœ¬
        body = soup.find('body')
        if body:
            return body.get_text().strip()
        
        return soup.get_text().strip()
    
    def _extract_smart_author(self, soup) -> str:
        """æ™ºèƒ½ä½œè€…æå–"""
        author_selectors = [
            '.author',
            '.username',
            '.user-name',
            '[data-author]',
            '.post-author',
            '.by-author'
        ]
        
        for selector in author_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                if text:
                    return text
        
        return ''
    
    def _extract_publish_time(self, soup) -> str:
        """æå–å‘å¸ƒæ—¶é—´"""
        time_selectors = [
            'time',
            '.time',
            '.date',
            '.publish-time',
            '[datetime]'
        ]
        
        for selector in time_selectors:
            elements = soup.select(selector)
            for element in elements:
                # å°è¯•è·å–datetimeå±æ€§
                datetime_attr = element.get('datetime')
                if datetime_attr:
                    return datetime_attr
                
                # è·å–æ–‡æœ¬å†…å®¹
                text = element.get_text().strip()
                if text:
                    return text
        
        return ''
    
    def _extract_tags(self, soup) -> List[str]:
        """æå–æ ‡ç­¾"""
        tags = []
        tag_selectors = [
            '.tag',
            '.tags a',
            '.hashtag',
            '[data-tag]'
        ]
        
        for selector in tag_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                if text and text not in tags:
                    tags.append(text)
        
        return tags[:10]  # é™åˆ¶æ ‡ç­¾æ•°é‡
    
    def _extract_engagement_data(self, soup) -> Dict[str, int]:
        """æå–äº’åŠ¨æ•°æ®"""
        engagement = {}
        
        # æŸ¥æ‰¾åŒ…å«æ•°å­—çš„å…ƒç´ ï¼Œå¯èƒ½æ˜¯äº’åŠ¨æ•°æ®
        import re
        number_pattern = re.compile(r'\d+[kKwWä¸‡åƒç™¾å]?')
        
        engagement_selectors = [
            '.like', '.likes',
            '.share', '.shares',
            '.comment', '.comments',
            '.view', '.views'
        ]
        
        for selector in engagement_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                numbers = number_pattern.findall(text)
                if numbers:
                    key = selector.replace('.', '').replace('s', '')  # æ ‡å‡†åŒ–é”®å
                    engagement[key] = self._parse_number(numbers[0])
        
        return engagement
    
    def _extract_media_urls(self, soup) -> Dict[str, List[str]]:
        """æå–åª’ä½“URL"""
        images = []
        videos = []
        
        # æå–å›¾ç‰‡
        for img in soup.find_all('img', src=True):
            src = img['src']
            if src and not src.startswith('data:'):  # æ’é™¤base64å›¾ç‰‡
                images.append(src)
        
        # æå–è§†é¢‘
        for video in soup.find_all('video', src=True):
            videos.append(video['src'])
        
        for source in soup.find_all('source', src=True):
            videos.append(source['src'])
        
        return {
            'images': images[:10],  # é™åˆ¶æ•°é‡
            'videos': videos[:5]
        }
    
    def _parse_number(self, text: str) -> int:
        """è§£ææ•°å­—æ–‡æœ¬ï¼ˆæ”¯æŒkã€wç­‰å•ä½ï¼‰"""
        try:
            text = text.lower().replace(',', '')
            if 'k' in text:
                return int(float(text.replace('k', '')) * 1000)
            elif 'w' in text or 'ä¸‡' in text:
                return int(float(text.replace('w', '').replace('ä¸‡', '')) * 10000)
            else:
                return int(text)
        except:
            return 0
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ''
        
        import re
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text.strip())
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
        return text
    
    def _clean_tags(self, tags) -> List[str]:
        """æ¸…ç†æ ‡ç­¾åˆ—è¡¨"""
        if not tags:
            return []
        
        cleaned_tags = []
        for tag in tags:
            if isinstance(tag, str):
                cleaned_tag = self._clean_text(tag)
                if cleaned_tag and cleaned_tag not in cleaned_tags:
                    cleaned_tags.append(cleaned_tag)
        
        return cleaned_tags[:10]  # é™åˆ¶æ•°é‡
    
    def _normalize_engagement(self, engagement) -> Dict[str, int]:
        """æ ‡å‡†åŒ–äº’åŠ¨æ•°æ®"""
        if not isinstance(engagement, dict):
            return {}
        
        normalized = {}
        for key, value in engagement.items():
            try:
                if isinstance(value, (int, float)):
                    normalized[key] = int(value)
                elif isinstance(value, str):
                    normalized[key] = self._parse_number(value)
            except:
                continue
        
        return normalized
    
    def _normalize_media_urls(self, media_urls) -> Dict[str, List[str]]:
        """æ ‡å‡†åŒ–åª’ä½“URL"""
        if not isinstance(media_urls, dict):
            return {'images': [], 'videos': []}
        
        result = {'images': [], 'videos': []}
        
        for key in ['images', 'videos']:
            urls = media_urls.get(key, [])
            if isinstance(urls, list):
                valid_urls = []
                for url in urls:
                    if isinstance(url, str) and url.strip():
                        valid_urls.append(url.strip())
                result[key] = valid_urls[:10]  # é™åˆ¶æ•°é‡
        
        return result
    
    def _convert_crawl4ai_result(self, crawl4ai_result: Dict[str, Any], url: str) -> CrawlResult:
        """å°†Crawl4AIç»“æœè½¬æ¢ä¸ºCrawlResultæ ¼å¼ï¼ŒåŒ…å«å¢å¼ºçš„æ•°æ®å¤„ç†"""
        try:
            # å¤„ç†é“¾æ¥å’Œå›¾ç‰‡ï¼Œç¡®ä¿æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
            links = self._extract_urls_from_result(crawl4ai_result.get("links", []))
            images = self._extract_urls_from_result(crawl4ai_result.get("images", []))
            
            # å¦‚æœæ²¡æœ‰ç›´æ¥çš„linkså’Œimagesï¼Œå°è¯•ä»media_urlsè·å–
            if not links or not images:
                media_urls = crawl4ai_result.get("media_urls", {})
                if isinstance(media_urls, dict):
                    if not links:
                        links = self._extract_urls_from_result(media_urls.get("images", []))
                    if not images:
                        images = self._extract_urls_from_result(media_urls.get("videos", []))
            
            # æ•°æ®è´¨é‡éªŒè¯å’Œæ¸…ç†
            title = self._clean_text(crawl4ai_result.get("title", ""))
            content = crawl4ai_result.get("content", "")
            
            # å¦‚æœå…³é”®æ•°æ®ç¼ºå¤±ï¼Œè®°å½•è­¦å‘Š
            if not title and not content:
                logger.warning(f"Both title and content are empty for URL: {url}")
            
            # å‡†å¤‡å†…å®¹JSON
            content_json = json.dumps(crawl4ai_result, ensure_ascii=False, default=str)
            
            return CrawlResult(
                task_id="",  # Will be set by caller
                session_id="",  # Will be set by caller
                url=url,
                status=CrawlTaskStatus.COMPLETED,
                title=title,
                content=content_json,
                links=links,
                images=images,
                metadata={
                    **crawl4ai_result.get("metadata", {}),
                    "data_quality_score": self._calculate_data_quality_score(crawl4ai_result)
                },
                created_at=datetime.now(timezone.utc)
            )
        except Exception as e:
            logger.error(f"Failed to convert Crawl4AI result: {e}")
            # è¿”å›åŸºç¡€ç»“æœ
            return CrawlResult(
                task_id="",
                session_id="",
                url=url,
                status=CrawlTaskStatus.COMPLETED,
                title=crawl4ai_result.get("title", ""),
                content=json.dumps(crawl4ai_result, ensure_ascii=False, default=str),
                links=[],
                images=[],
                metadata={"conversion_error": str(e)},
                created_at=datetime.now(timezone.utc)
            )
    
    def _merge_extraction_results(self, primary: Dict[str, Any], fallback: Dict[str, Any]) -> Dict[str, Any]:
         """åˆå¹¶ä¸¤ä¸ªæå–ç»“æœï¼Œä¼˜å…ˆä½¿ç”¨è´¨é‡æ›´å¥½çš„æ•°æ®"""
         merged = primary.copy()
         
         # åˆå¹¶æ ‡é¢˜ï¼ˆä¼˜å…ˆä½¿ç”¨æ›´é•¿çš„æœ‰æ•ˆæ ‡é¢˜ï¼‰
         primary_title = primary.get('title', '').strip()
         fallback_title = fallback.get('title', '').strip()
         if len(fallback_title) > len(primary_title) and len(fallback_title) > 5:
             merged['title'] = fallback_title
         
         # åˆå¹¶å†…å®¹ï¼ˆä¼˜å…ˆä½¿ç”¨æ›´é•¿çš„æœ‰æ•ˆå†…å®¹ï¼‰
         primary_content = primary.get('content', '').strip()
         fallback_content = fallback.get('content', '').strip()
         if len(fallback_content) > len(primary_content) and len(fallback_content) > 50:
             merged['content'] = fallback_content
         
         # åˆå¹¶ä½œè€…ä¿¡æ¯
         if not merged.get('author') and fallback.get('author'):
             merged['author'] = fallback['author']
         
         # åˆå¹¶æ—¶é—´ä¿¡æ¯
         if not merged.get('publish_time') and fallback.get('publish_time'):
             merged['publish_time'] = fallback['publish_time']
         
         # åˆå¹¶æ ‡ç­¾
         primary_tags = merged.get('tags', [])
         fallback_tags = fallback.get('tags', [])
         if isinstance(fallback_tags, list) and fallback_tags:
             all_tags = list(primary_tags) if isinstance(primary_tags, list) else []
             for tag in fallback_tags:
                 if tag not in all_tags:
                     all_tags.append(tag)
             merged['tags'] = all_tags[:10]
         
         # åˆå¹¶äº’åŠ¨æ•°æ®
         primary_engagement = merged.get('engagement', {})
         fallback_engagement = fallback.get('engagement', {})
         if isinstance(fallback_engagement, dict) and fallback_engagement:
             merged_engagement = primary_engagement.copy() if isinstance(primary_engagement, dict) else {}
             for key, value in fallback_engagement.items():
                 if key not in merged_engagement or not merged_engagement[key]:
                     merged_engagement[key] = value
             merged['engagement'] = merged_engagement
         
         # åˆå¹¶åª’ä½“URL
         primary_media = merged.get('media_urls', {})
         fallback_media = fallback.get('media_urls', {})
         if isinstance(fallback_media, dict):
             merged_media = primary_media.copy() if isinstance(primary_media, dict) else {}
             for media_type in ['images', 'videos']:
                 primary_urls = merged_media.get(media_type, [])
                 fallback_urls = fallback_media.get(media_type, [])
                 if isinstance(fallback_urls, list) and fallback_urls:
                     all_urls = list(primary_urls) if isinstance(primary_urls, list) else []
                     for url in fallback_urls:
                         if url not in all_urls:
                             all_urls.append(url)
                     merged_media[media_type] = all_urls[:10]
             merged['media_urls'] = merged_media
         
         # åˆå¹¶é“¾æ¥å’Œå›¾ç‰‡ï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼‰
         if not merged.get('links') and fallback.get('links'):
             merged['links'] = fallback['links']
         if not merged.get('images') and fallback.get('images'):
             merged['images'] = fallback['images']
         
         return merged
    
    def _extract_urls_from_result(self, urls_data) -> List[str]:
         """ä»ç»“æœä¸­æå–URLå­—ç¬¦ä¸²åˆ—è¡¨"""
         if not urls_data:
             return []
         
         if isinstance(urls_data, list):
             result = []
             for item in urls_data:
                 if isinstance(item, str):
                     result.append(item)
                 elif isinstance(item, dict) and item.get('url'):
                     result.append(item['url'])
             return result[:10]  # é™åˆ¶æ•°é‡
         
         return []
    
    def _convert_enhanced_result(self, enhanced_result: Dict[str, Any], url: str) -> CrawlResult:
         """å°†å¢å¼ºfallbackç»“æœè½¬æ¢ä¸ºCrawlResultæ ¼å¼"""
         try:
             # æå–å’Œæ¸…ç†æ•°æ®
             title = self._clean_text(enhanced_result.get('title', ''))
             content = enhanced_result.get('content', '')
             
             # å¤„ç†åª’ä½“URL
             media_urls = enhanced_result.get('media_urls', {})
             links = self._extract_urls_from_result(media_urls.get('images', []))
             images = self._extract_urls_from_result(media_urls.get('videos', []))
             
             # å¦‚æœæ²¡æœ‰åª’ä½“URLï¼Œå°è¯•ä»linkså’Œimageså­—æ®µè·å–
             if not links:
                 links = self._extract_urls_from_result(enhanced_result.get('links', []))
             if not images:
                 images = self._extract_urls_from_result(enhanced_result.get('images', []))
             
             # å‡†å¤‡å†…å®¹JSON
             content_json = json.dumps(enhanced_result, ensure_ascii=False, default=str)
             
             # è®¡ç®—è´¨é‡åˆ†æ•°
             quality_score = self._calculate_data_quality_score(enhanced_result)
             
             return CrawlResult(
                 task_id="",
                 session_id="",
                 url=url,
                 status=CrawlTaskStatus.COMPLETED,
                 title=title,
                 content=content_json,
                 links=links,
                 images=images,
                 metadata={
                     "extraction_method": "enhanced_fallback",
                     "data_quality_score": quality_score,
                     "author": enhanced_result.get('author', ''),
                     "publish_time": enhanced_result.get('publish_time', ''),
                     "tags": enhanced_result.get('tags', []),
                     "engagement": enhanced_result.get('engagement', {})
                 },
                 created_at=datetime.now(timezone.utc)
             )
             
         except Exception as e:
             logger.error(f"Failed to convert enhanced result: {e}")
             return CrawlResult(
                 task_id="",
                 session_id="",
                 url=url,
                 status=CrawlTaskStatus.FAILED,
                 title="",
                 content=json.dumps({"error": str(e)}, ensure_ascii=False),
                 links=[],
                 images=[],
                 metadata={"conversion_error": str(e), "data_quality_score": 0.0},
                 created_at=datetime.now(timezone.utc)
             )
    
    def _calculate_data_quality_score(self, data: Dict[str, Any]) -> float:
         """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°"""
         score = 0.0
         
         # æ ‡é¢˜è´¨é‡ (30%)
         title = data.get('title', '')
         if title:
             score += 0.3 * min(len(title) / 50, 1.0)
         
         # å†…å®¹è´¨é‡ (40%)
         content = data.get('content', '')
         if content:
             score += 0.4 * min(len(content) / 200, 1.0)
         
         # ä½œè€…ä¿¡æ¯ (10%)
         if data.get('author'):
             score += 0.1
         
         # æ—¶é—´ä¿¡æ¯ (10%)
         if data.get('publish_time'):
             score += 0.1
         
         # åª’ä½“æ–‡ä»¶ (10%)
         media_urls = data.get('media_urls', {})
         if media_urls.get('images') or media_urls.get('videos'):
             score += 0.1
         
         return round(score, 2)
    
    async def cleanup(self):
        """Cleanup Crawl4AI resources"""
        if self.crawler:
            try:
                # In newer versions of Crawl4AI, cleanup is handled automatically
                # or through context managers
                logger.info("Crawl4AI crawler cleaned up")
            except Exception as e:
                logger.warning(f"Error cleaning up Crawl4AI crawler: {e}")
            finally:
                self.crawler = None
    
    async def get_crawl_task(self, task_id: str) -> Optional[Dict[str, Any]]:
        """Get crawl task by ID"""
        task = await self.crawl_tasks.find_one({"task_id": task_id})
        return convert_objectid_to_str(task) if task else None
    
    async def list_crawl_tasks(
        self,
        user_id: str,
        platform: Optional[PlatformType] = None,
        status: Optional[CrawlTaskStatus] = None,
        limit: int = 50,
        offset: int = 0
    ) -> List[Dict[str, Any]]:
        """List crawl tasks for a user"""
        query = {"user_id": user_id}
        
        if platform:
            query["platform"] = platform.value
        
        if status:
            query["status"] = status.value
        
        cursor = self.crawl_tasks.find(query)
        cursor = cursor.sort("created_at", -1).skip(offset).limit(limit)
        
        tasks = await cursor.to_list(length=limit)
        return [convert_objectid_to_str(task) for task in tasks]
    
    async def delete_crawl_task(self, task_id: str, user_id: str) -> bool:
        """Delete a crawl task"""
        result = await self.crawl_tasks.delete_one({
            "task_id": task_id,
            "user_id": user_id
        })
        return result.deleted_count > 0
    
    async def cleanup_old_tasks(self, days: int = 30) -> int:
        """Clean up old crawl tasks"""
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
        
        result = await self.crawl_tasks.delete_many({
            "created_at": {"$lt": cutoff_date}
        })
        
        logger.info(f"Cleaned up {result.deleted_count} old crawl tasks")
        return result.deleted_count
    
    async def get_crawl_statistics(self, user_id: Optional[str] = None) -> Dict[str, Any]:
        """Get crawl statistics"""
        query = {}
        if user_id:
            query["user_id"] = user_id
        
        pipeline = [
            {"$match": query},
            {
                "$group": {
                    "_id": "$status",
                    "count": {"$sum": 1}
                }
            }
        ]
        
        status_counts = {}
        async for doc in self.crawl_tasks.aggregate(pipeline):
            status_counts[doc["_id"]] = doc["count"]
        
        # Platform statistics
        platform_pipeline = [
            {"$match": query},
            {
                "$group": {
                    "_id": "$platform",
                    "count": {"$sum": 1}
                }
            }
        ]
        
        platform_counts = {}
        async for doc in self.crawl_tasks.aggregate(platform_pipeline):
            platform_counts[doc["_id"]] = doc["count"]
        
        total_tasks = await self.crawl_tasks.count_documents(query)
        
        return {
            "total_tasks": total_tasks,
            "status_distribution": status_counts,
            "platform_distribution": platform_counts,
            "generated_at": datetime.now(timezone.utc).isoformat()
        }
    
    def _classify_error(self, error_message: str) -> str:
        """é”™è¯¯åˆ†ç±»"""
        error_message_lower = error_message.lower()
        
        for error_type, patterns in self.error_patterns.items():
            for pattern in patterns:
                if pattern.lower() in error_message_lower:
                    return error_type
        
        return "unknown"
    

    
    async def _check_login_status(self, page: Page, platform: PlatformType) -> bool:
        """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
        try:
            if platform not in self.login_indicators:
                return True  # æœªé…ç½®æ£€æµ‹è§„åˆ™ï¼Œå‡è®¾å·²ç™»å½•
            
            indicators = self.login_indicators[platform]
            
            # æ£€æŸ¥ç™»å½•çŠ¶æ€æŒ‡ç¤ºå™¨
            for selector in indicators["logged_in_selectors"]:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        logger.debug(f"æ‰¾åˆ°ç™»å½•çŠ¶æ€æŒ‡ç¤ºå™¨: {selector}")
                        return True
                except Exception:
                    continue
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
            for selector in indicators["login_required_selectors"]:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        logger.warning(f"æ‰¾åˆ°ç™»å½•è¦æ±‚æŒ‡ç¤ºå™¨: {selector}")
                        return False
                except Exception:
                    continue
            
            # é»˜è®¤å‡è®¾å·²ç™»å½•
            return True
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶å‡è®¾å·²ç™»å½•ï¼Œé¿å…é˜»å¡
    
    async def _detect_anti_crawler(self, html_content: str, page: Page) -> bool:
        """æ£€æµ‹åçˆ¬è™«æœºåˆ¶"""
        try:
            # æ£€æŸ¥HTMLå†…å®¹ä¸­çš„åçˆ¬è™«å…³é”®è¯
            anti_crawler_keywords = [
                "captcha", "verification", "robot", "blocked", 
                "challenge", "è¯·å®ŒæˆéªŒè¯", "äººæœºéªŒè¯", "æ»‘åŠ¨éªŒè¯"
            ]
            
            html_lower = html_content.lower()
            for keyword in anti_crawler_keywords:
                if keyword in html_lower:
                    logger.warning(f"åœ¨HTMLä¸­æ£€æµ‹åˆ°åçˆ¬è™«å…³é”®è¯: {keyword}")
                    return True
            
            # æ£€æŸ¥é¡µé¢æ ‡é¢˜
            try:
                title = await page.title()
                title_lower = title.lower()
                for keyword in anti_crawler_keywords:
                    if keyword in title_lower:
                        logger.warning(f"åœ¨é¡µé¢æ ‡é¢˜ä¸­æ£€æµ‹åˆ°åçˆ¬è™«å…³é”®è¯: {keyword}")
                        return True
            except Exception:
                pass
            
            # æ£€æŸ¥ç‰¹å®šçš„åçˆ¬è™«å…ƒç´ 
            anti_crawler_selectors = [
                ".captcha", "#captcha", ".verification", ".challenge",
                "[class*='captcha']", "[id*='captcha']", "[class*='verify']"
            ]
            
            for selector in anti_crawler_selectors:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        logger.warning(f"æ£€æµ‹åˆ°åçˆ¬è™«å…ƒç´ : {selector}")
                        return True
                except Exception:
                    continue
            
            return False
            
        except Exception as e:
            logger.error(f"åçˆ¬è™«æ£€æµ‹å¤±è´¥: {e}")
            return False


class ContinuousCrawlService:
    """æŒç»­çˆ¬å–æœåŠ¡"""
    
    def __init__(
        self,
        db: AsyncIOMotorDatabase,
        session_manager: SessionManager,
        browser_manager: BrowserInstanceManager,
        cookie_store: CookieStore
    ):
        self.db = db
        self.session_manager = session_manager
        self.browser_manager = browser_manager
        self.cookie_store = cookie_store
        self.continuous_tasks = db.continuous_crawl_tasks
        self.manual_crawl_service = ManualCrawlService(
            db, session_manager, browser_manager, cookie_store
        )
        
        # æ´»è·ƒçš„æŒç»­çˆ¬å–ä»»åŠ¡
        self.active_tasks: Dict[str, asyncio.Task] = {}
        self.task_locks: Dict[str, asyncio.Lock] = {}
        
    async def start_continuous_crawl(
        self,
        session_id: str,
        user_id: str,
        url: str,
        platform: PlatformType,
        config: Optional[ContinuousCrawlConfig] = None
    ) -> str:
        """å¯åŠ¨æŒç»­çˆ¬å–ä»»åŠ¡"""
        try:
            # éªŒè¯ä¼šè¯
            session = await self.session_manager.validate_session(session_id)
            if not session or session.get('user_id') != user_id:
                raise ValueError("Invalid session")
            
            # ä½¿ç”¨é»˜è®¤é…ç½®
            if config is None:
                config = ContinuousCrawlConfig()
            
            # åˆ›å»ºä»»åŠ¡ID
            task_id = f"continuous_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}_{user_id[:8]}"
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒURLçš„æ´»è·ƒä»»åŠ¡
            existing_task = await self.continuous_tasks.find_one({
                "session_id": session_id,
                "url": url,
                "status": {"$in": [ContinuousTaskStatus.RUNNING.value, ContinuousTaskStatus.PAUSED.value]}
            })
            
            if existing_task:
                logger.warning(f"Continuous crawl task already exists for URL {url} in session {session_id}")
                return existing_task["task_id"]
            
            # åˆ›å»ºä»»åŠ¡æ–‡æ¡£
            task = ContinuousCrawlTask(
                task_id=task_id,
                session_id=session_id,
                user_id=user_id,
                url=url,
                platform=platform,
                status=ContinuousTaskStatus.RUNNING,
                config=config,
                created_at=datetime.now(timezone.utc),
                next_crawl_at=datetime.now(timezone.utc)
            )
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            await self.continuous_tasks.insert_one(task.dict())
            
            # å¯åŠ¨æŒç»­çˆ¬å–ä»»åŠ¡
            crawl_task = asyncio.create_task(self._continuous_crawl_loop(task_id))
            self.active_tasks[task_id] = crawl_task
            self.task_locks[task_id] = asyncio.Lock()
            
            logger.info(f"Started continuous crawl task {task_id} for URL {url}")
            return task_id
            
        except Exception as e:
            logger.error(f"Failed to start continuous crawl: {e}")
            raise
    
    async def stop_continuous_crawl(self, task_id: str, user_id: str) -> bool:
        """åœæ­¢æŒç»­çˆ¬å–ä»»åŠ¡"""
        try:
            # éªŒè¯ä»»åŠ¡æ‰€æœ‰æƒ
            task_doc = await self.continuous_tasks.find_one({
                "task_id": task_id,
                "user_id": user_id
            })
            
            if not task_doc:
                logger.warning(f"Continuous crawl task {task_id} not found for user {user_id}")
                return False
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            await self.continuous_tasks.update_one(
                {"task_id": task_id},
                {
                    "$set": {
                        "status": ContinuousTaskStatus.STOPPED.value,
                        "updated_at": datetime.now(timezone.utc)
                    }
                }
            )
            
            # å–æ¶ˆæ´»è·ƒä»»åŠ¡
            if task_id in self.active_tasks:
                self.active_tasks[task_id].cancel()
                del self.active_tasks[task_id]
            
            if task_id in self.task_locks:
                del self.task_locks[task_id]
            
            logger.info(f"Stopped continuous crawl task {task_id}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to stop continuous crawl task {task_id}: {e}")
            return False
    
    async def _continuous_crawl_loop(self, task_id: str):
        """æŒç»­çˆ¬å–å¾ªç¯"""
        try:
            while True:
                # è·å–ä»»åŠ¡çŠ¶æ€
                task_doc = await self.continuous_tasks.find_one({"task_id": task_id})
                if not task_doc:
                    logger.warning(f"Continuous crawl task {task_id} not found, stopping loop")
                    break
                
                task_status = ContinuousTaskStatus(task_doc["status"])
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
                if task_status in [ContinuousTaskStatus.STOPPED, ContinuousTaskStatus.ERROR]:
                    logger.info(f"Continuous crawl task {task_id} status is {task_status}, stopping loop")
                    break
                
                # å¦‚æœä»»åŠ¡æš‚åœï¼Œç­‰å¾…åç»§ç»­æ£€æŸ¥
                if task_status == ContinuousTaskStatus.PAUSED:
                    await asyncio.sleep(5)
                    continue
                
                # æ£€æŸ¥æ˜¯å¦åˆ°äº†çˆ¬å–æ—¶é—´
                now = datetime.now(timezone.utc)
                next_crawl_at = task_doc.get("next_crawl_at")
                if next_crawl_at:
                    # ç¡®ä¿ next_crawl_at æ˜¯å¸¦æ—¶åŒºçš„ datetime
                    if next_crawl_at.tzinfo is None:
                        next_crawl_at = next_crawl_at.replace(tzinfo=timezone.utc)
                    
                    if now < next_crawl_at:
                        sleep_time = (next_crawl_at - now).total_seconds()
                        await asyncio.sleep(min(sleep_time, 5))  # æœ€å¤šç¡çœ 5ç§’
                        continue
                
                # æ£€æŸ¥é¡µé¢åœç•™çŠ¶æ€
                if not await self._check_page_stay(task_doc):
                    logger.info(f"User left page for task {task_id}, stopping continuous crawl")
                    await self.stop_continuous_crawl(task_id, task_doc["user_id"])
                    break
                
                # æ‰§è¡Œçˆ¬å–
                await self._execute_continuous_crawl(task_id)
                
                # ç­‰å¾…ä¸‹æ¬¡çˆ¬å–é—´éš”
                config = ContinuousCrawlConfig(**task_doc["config"])
                await asyncio.sleep(min(config.crawl_interval_seconds, 60))  # æœ€å¤§é—´éš”60ç§’
                
        except asyncio.CancelledError:
            logger.info(f"Continuous crawl task {task_id} was cancelled")
        except Exception as e:
            logger.error(f"Error in continuous crawl loop for task {task_id}: {e}")
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºé”™è¯¯
            await self.continuous_tasks.update_one(
                {"task_id": task_id},
                {
                    "$set": {
                        "status": ContinuousTaskStatus.ERROR.value,
                        "last_error": str(e),
                        "updated_at": datetime.now(timezone.utc)
                    },
                    "$inc": {"error_count": 1}
                }
            )
        finally:
            # æ¸…ç†èµ„æº
            if task_id in self.active_tasks:
                del self.active_tasks[task_id]
            if task_id in self.task_locks:
                del self.task_locks[task_id]
    
    async def _check_page_stay(self, task_doc: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦è¿˜åœ¨é¡µé¢ä¸Š"""
        try:
            session_id = task_doc["session_id"]
            target_url = task_doc["url"]
            
            # è·å–ä¼šè¯çš„æµè§ˆå™¨å®ä¾‹
            session_instances = await self.browser_manager.get_session_instances(session_id)
            if not session_instances:
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒå®ä¾‹åœ¨ç›®æ ‡é¡µé¢
            for instance in session_instances:
                if not instance.get("is_active", False):
                    continue
                
                instance_id = instance.get("instance_id")
                if not instance_id or instance_id not in self.browser_manager.browsers:
                    continue
                
                # æ£€æŸ¥é¡µé¢URL
                main_page_key = f"{instance_id}_main"
                page = self.browser_manager.pages.get(main_page_key)
                if page:
                    try:
                        current_url = page.url
                        # ç®€å•çš„URLåŒ¹é…æ£€æŸ¥
                        if self._urls_match(current_url, target_url):
                            return True
                    except Exception as e:
                        logger.warning(f"Failed to get page URL for instance {instance_id}: {e}")
                        continue
            
            return False
            
        except Exception as e:
            logger.error(f"Failed to check page stay: {e}")
            return False
    
    def _urls_match(self, current_url: str, target_url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦åŒ¹é…"""
        try:
            # è§£æURL
            current_parsed = urlparse(current_url)
            target_parsed = urlparse(target_url)
            
            # æ¯”è¾ƒåŸŸåå’Œè·¯å¾„
            return (
                current_parsed.netloc == target_parsed.netloc and
                current_parsed.path == target_parsed.path
            )
        except Exception:
            return current_url == target_url
    
    async def _execute_continuous_crawl(self, task_id: str):
        """æ‰§è¡Œå•æ¬¡æŒç»­çˆ¬å–"""
        async with self.task_locks.get(task_id, asyncio.Lock()):
            try:
                # è·å–ä»»åŠ¡ä¿¡æ¯
                task_doc = await self.continuous_tasks.find_one({"task_id": task_id})
                if not task_doc:
                    return
                
                config = ContinuousCrawlConfig(**task_doc["config"])
                
                # åˆ›å»ºä¸´æ—¶çš„æ‰‹åŠ¨çˆ¬å–ä»»åŠ¡
                manual_request = ManualCrawlRequest(
                    session_id=task_doc["session_id"],
                    url=task_doc["url"],
                    platform=PlatformType(task_doc["platform"]),
                    config=task_doc.get("manual_config", {})
                )
                
                # åˆ›å»ºæ‰‹åŠ¨çˆ¬å–ä»»åŠ¡
                manual_task_id = await self.manual_crawl_service.create_crawl_task(
                    manual_request,
                    task_doc["user_id"]
                )
                
                # æ‰§è¡Œçˆ¬å–
                result = await self.manual_crawl_service.execute_crawl_task(manual_task_id)
                
                # å¤„ç†çˆ¬å–ç»“æœ
                if result.success:
                    await self._process_crawl_result(task_id, result, config)
                else:
                    logger.warning(f"Crawl failed for continuous task {task_id}: {result.metadata.get('error')}")
                    await self._handle_crawl_error(task_id, result.metadata.get('error', 'Unknown error'))
                
            except Exception as e:
                logger.error(f"Failed to execute continuous crawl for task {task_id}: {e}")
                await self._handle_crawl_error(task_id, str(e))
    
    async def _process_crawl_result(
        self,
        task_id: str,
        result: CrawlResult,
        config: ContinuousCrawlConfig
    ):
        """å¤„ç†çˆ¬å–ç»“æœ"""
        try:
            # è®¡ç®—å†…å®¹å“ˆå¸Œ
            content_hash = self._calculate_content_hash(result.content)
            
            # è·å–å½“å‰ä»»åŠ¡çŠ¶æ€
            task_doc = await self.continuous_tasks.find_one({"task_id": task_id})
            if not task_doc:
                return
            
            # æ£€æŸ¥å†…å®¹å»é‡
            is_duplicate = False
            if config.enable_deduplication:
                last_hash = task_doc.get("last_content_hash")
                if last_hash == content_hash:
                    is_duplicate = True
                    logger.debug(f"Duplicate content detected for task {task_id}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            now = datetime.now(timezone.utc)
            next_crawl_at = now + timedelta(seconds=config.crawl_interval_seconds)
            
            update_data = {
                "last_crawl_at": now,
                "next_crawl_at": next_crawl_at,
                "updated_at": now,
                "$inc": {"crawl_count": 1}
            }
            
            if not is_duplicate:
                update_data["last_content_hash"] = content_hash
                update_data["$push"] = {
                    "content_hashes": {
                        "$each": [content_hash],
                        "$slice": -10  # åªä¿ç•™æœ€è¿‘10ä¸ªå“ˆå¸Œ
                    }
                }
            
            await self.continuous_tasks.update_one(
                {"task_id": task_id},
                update_data
            )
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢ï¼ˆæ— å˜åŒ–åœæ­¢æ¡ä»¶ï¼‰
            if config.stop_on_no_changes and is_duplicate:
                crawl_count = task_doc.get("crawl_count", 0)
                if crawl_count >= 3:  # è¿ç»­3æ¬¡æ— å˜åŒ–åˆ™åœæ­¢
                    logger.info(f"Stopping continuous crawl task {task_id} due to no content changes")
                    await self.stop_continuous_crawl(task_id, task_doc["user_id"])
            
            # æ£€æŸ¥æœ€å¤§çˆ¬å–æ¬¡æ•°
            if config.max_crawls and task_doc.get("crawl_count", 0) >= config.max_crawls:
                logger.info(f"Stopping continuous crawl task {task_id} due to max crawls reached")
                await self.stop_continuous_crawl(task_id, task_doc["user_id"])
            
        except Exception as e:
            logger.error(f"Failed to process crawl result for task {task_id}: {e}")
    
    def _calculate_content_hash(self, content: Dict[str, Any]) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
        try:
            # æå–ä¸»è¦å†…å®¹å­—æ®µ
            main_content = {
                "title": content.get("title", ""),
                "content": content.get("content", ""),
                "author": content.get("author", "")
            }
            
            # åºåˆ—åŒ–å¹¶è®¡ç®—å“ˆå¸Œ
            content_str = json.dumps(main_content, sort_keys=True, ensure_ascii=False)
            return hashlib.md5(content_str.encode('utf-8')).hexdigest()
        except Exception as e:
            logger.warning(f"Failed to calculate content hash: {e}")
            return ""
    
    async def _handle_crawl_error(self, task_id: str, error_message: str):
        """å¤„ç†çˆ¬å–é”™è¯¯"""
        try:
            await self.continuous_tasks.update_one(
                {"task_id": task_id},
                {
                    "$set": {
                        "last_error": error_message,
                        "updated_at": datetime.now(timezone.utc)
                    },
                    "$inc": {"error_count": 1}
                }
            )
            
            # å¦‚æœé”™è¯¯æ¬¡æ•°è¿‡å¤šï¼Œåœæ­¢ä»»åŠ¡
            task_doc = await self.continuous_tasks.find_one({"task_id": task_id})
            if task_doc and task_doc.get("error_count", 0) >= 5:
                logger.warning(f"Stopping continuous crawl task {task_id} due to too many errors")
                await self.continuous_tasks.update_one(
                    {"task_id": task_id},
                    {"$set": {"status": ContinuousTaskStatus.ERROR.value}}
                )
                
        except Exception as e:
            logger.error(f"Failed to handle crawl error for task {task_id}: {e}")
    
    async def get_continuous_task(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æŒç»­çˆ¬å–ä»»åŠ¡"""
        task = await self.continuous_tasks.find_one({"task_id": task_id})
        return convert_objectid_to_str(task) if task else None
    
    async def list_continuous_tasks(
        self,
        user_id: str,
        status: Optional[ContinuousTaskStatus] = None,
        limit: int = 50,
        offset: int = 0
    ) -> List[Dict[str, Any]]:
        """åˆ—å‡ºç”¨æˆ·çš„æŒç»­çˆ¬å–ä»»åŠ¡"""
        query = {"user_id": user_id}
        
        if status:
            query["status"] = status.value
        
        cursor = self.continuous_tasks.find(query)
        cursor = cursor.sort("created_at", -1).skip(offset).limit(limit)
        
        tasks = await cursor.to_list(length=limit)
        return [convert_objectid_to_str(task) for task in tasks]
    
    async def cleanup_stopped_tasks(self, days: int = 7) -> int:
        """æ¸…ç†å·²åœæ­¢çš„ä»»åŠ¡"""
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
        
        result = await self.continuous_tasks.delete_many({
            "status": {"$in": [ContinuousTaskStatus.STOPPED.value, ContinuousTaskStatus.ERROR.value]},
            "updated_at": {"$lt": cutoff_date}
        })
        
        logger.info(f"Cleaned up {result.deleted_count} stopped continuous crawl tasks")
        return result.deleted_count
    
    async def _check_login_status(self, page: Page, platform: PlatformType) -> bool:
        """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
        try:
            if platform not in self.login_indicators:
                return True  # æœªé…ç½®æ£€æµ‹è§„åˆ™ï¼Œå‡è®¾å·²ç™»å½•
            
            indicators = self.login_indicators[platform]
            
            # æ£€æŸ¥ç™»å½•çŠ¶æ€æŒ‡ç¤ºå™¨
            for selector in indicators["logged_in_selectors"]:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        logger.debug(f"æ‰¾åˆ°ç™»å½•çŠ¶æ€æŒ‡ç¤ºå™¨: {selector}")
                        return True
                except Exception:
                    continue
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
            for selector in indicators["login_required_selectors"]:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        logger.warning(f"æ‰¾åˆ°ç™»å½•è¦æ±‚æŒ‡ç¤ºå™¨: {selector}")
                        return False
                except Exception:
                    continue
            
            # é»˜è®¤å‡è®¾å·²ç™»å½•
            return True
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶å‡è®¾å·²ç™»å½•ï¼Œé¿å…é˜»å¡
    
    async def _attempt_auto_login(self, page: Page, platform: PlatformType):
        """å°è¯•è‡ªåŠ¨é‡æ–°ç™»å½•"""
        try:
            if platform not in self.login_indicators:
                logger.warning(f"å¹³å° {platform.value} æœªé…ç½®è‡ªåŠ¨ç™»å½•")
                return
            
            login_url = self.login_indicators[platform]["login_url"]
            current_url = page.url
            
            # å¦‚æœå½“å‰ä¸åœ¨ç™»å½•é¡µé¢ï¼Œå¯¼èˆªåˆ°ç™»å½•é¡µé¢
            if login_url not in current_url:
                logger.info(f"å¯¼èˆªåˆ°ç™»å½•é¡µé¢: {login_url}")
                await page.goto(login_url, wait_until="networkidle")
                await asyncio.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„è‡ªåŠ¨ç™»å½•é€»è¾‘
            # ç›®å‰åªæ˜¯å¯¼èˆªåˆ°ç™»å½•é¡µé¢ï¼Œå®é™…ç™»å½•éœ€è¦ç”¨æˆ·æ‰‹åŠ¨å®Œæˆ
            logger.info(f"å·²å¯¼èˆªåˆ°ç™»å½•é¡µé¢ï¼Œè¯·æ‰‹åŠ¨å®Œæˆç™»å½•: {login_url}")
            
        except Exception as e:
            logger.error(f"è‡ªåŠ¨ç™»å½•å°è¯•å¤±è´¥: {e}")
    
    def _classify_error(self, error_message: str) -> str:
        """é”™è¯¯åˆ†ç±»"""
        error_message_lower = error_message.lower()
        
        for error_type, patterns in self.error_patterns.items():
            for pattern in patterns:
                if pattern.lower() in error_message_lower:
                    return error_type
        
        return "unknown"
    
    async def _detect_anti_crawler(self, html_content: str, page: Page) -> bool:
        """æ£€æµ‹åçˆ¬è™«æœºåˆ¶"""
        try:
            # æ£€æŸ¥HTMLå†…å®¹ä¸­çš„åçˆ¬è™«å…³é”®è¯
            anti_crawler_keywords = [
                "captcha", "verification", "robot", "blocked", 
                "challenge", "è¯·å®ŒæˆéªŒè¯", "äººæœºéªŒè¯", "æ»‘åŠ¨éªŒè¯"
            ]
            
            html_lower = html_content.lower()
            for keyword in anti_crawler_keywords:
                if keyword in html_lower:
                    logger.warning(f"åœ¨HTMLä¸­æ£€æµ‹åˆ°åçˆ¬è™«å…³é”®è¯: {keyword}")
                    return True
            
            # æ£€æŸ¥é¡µé¢æ ‡é¢˜
            try:
                title = await page.title()
                title_lower = title.lower()
                for keyword in anti_crawler_keywords:
                    if keyword in title_lower:
                        logger.warning(f"åœ¨é¡µé¢æ ‡é¢˜ä¸­æ£€æµ‹åˆ°åçˆ¬è™«å…³é”®è¯: {keyword}")
                        return True
            except Exception:
                pass
            
            # æ£€æŸ¥ç‰¹å®šçš„åçˆ¬è™«å…ƒç´ 
            anti_crawler_selectors = [
                ".captcha", "#captcha", ".verification", ".challenge",
                "[class*='captcha']", "[id*='captcha']", "[class*='verify']"
            ]
            
            for selector in anti_crawler_selectors:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        logger.warning(f"æ£€æµ‹åˆ°åçˆ¬è™«å…ƒç´ : {selector}")
                        return True
                except Exception:
                    continue
            
            return False
            
        except Exception as e:
            logger.error(f"åçˆ¬è™«æ£€æµ‹å¤±è´¥: {e}")
            return False