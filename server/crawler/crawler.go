package crawler

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"sync"
	"time"

	"go.mongodb.org/mongo-driver/bson"
	"go.mongodb.org/mongo-driver/bson/primitive"
	"go.mongodb.org/mongo-driver/mongo"

	"newshub/config"
	"newshub/models"
)

const PYTHON_CRAWLER_URL = "http://localhost:8001"

// ScheduledCrawlerService æ™ºèƒ½å®šæ—¶çˆ¬è™«æœåŠ¡
type ScheduledCrawlerService struct {
	db        *mongo.Database
	isRunning bool
	stopChan  chan bool
	wg        sync.WaitGroup
}

// CrawlRequest Pythonçˆ¬è™«è¯·æ±‚ç»“æ„
type CrawlRequest struct {
	Platform   string `json:"platform"`
	CreatorURL string `json:"creator_url"`
	Limit      int    `json:"limit"`
}

// CrawlResponse Pythonçˆ¬è™«å“åº”ç»“æ„
type CrawlResponse struct {
	Posts []PostData `json:"posts"`
	Total int        `json:"total"`
}

// PostData çˆ¬å–åˆ°çš„å¸–å­æ•°æ®
type PostData struct {
	Title       string    `json:"title"`
	Content     string    `json:"content"`
	Author      string    `json:"author"`
	Platform    string    `json:"platform"`
	URL         string    `json:"url"`
	PublishedAt time.Time `json:"published_at"`
	Tags        []string  `json:"tags"`
	Images      []string  `json:"images"`
	VideoURL    string    `json:"video_url,omitempty"`
	OriginID    string    `json:"origin_id,omitempty"`
}

// NewScheduledCrawlerService åˆ›å»ºæ–°çš„å®šæ—¶çˆ¬è™«æœåŠ¡
func NewScheduledCrawlerService() *ScheduledCrawlerService {
	return &ScheduledCrawlerService{
		db:       config.GetDB(),
		stopChan: make(chan bool),
	}
}

// Start å¯åŠ¨å®šæ—¶çˆ¬è™«æœåŠ¡
func (scs *ScheduledCrawlerService) Start() {
	if scs.isRunning {
		log.Println("å®šæ—¶çˆ¬è™«æœåŠ¡å·²åœ¨è¿è¡Œä¸­")
		return
	}

	scs.isRunning = true
	log.Println("ğŸš€ å¯åŠ¨æ™ºèƒ½å®šæ—¶çˆ¬è™«æœåŠ¡...")

	// ç«‹å³æ‰§è¡Œä¸€æ¬¡åˆå§‹çˆ¬å–
	go scs.performScheduledCrawl()

	// å¯åŠ¨ä¸»è°ƒåº¦å¾ªç¯
	scs.wg.Add(1)
	go scs.schedulerLoop()

	log.Println("âœ… å®šæ—¶çˆ¬è™«æœåŠ¡å¯åŠ¨æˆåŠŸ")
}

// Stop åœæ­¢å®šæ—¶çˆ¬è™«æœåŠ¡
func (scs *ScheduledCrawlerService) Stop() {
	if !scs.isRunning {
		return
	}

	log.Println("â¹ï¸ åœæ­¢å®šæ—¶çˆ¬è™«æœåŠ¡...")
	scs.stopChan <- true
	scs.wg.Wait()
	scs.isRunning = false
	log.Println("âœ… å®šæ—¶çˆ¬è™«æœåŠ¡å·²åœæ­¢")
}

// schedulerLoop ä¸»è°ƒåº¦å¾ªç¯
func (scs *ScheduledCrawlerService) schedulerLoop() {
	defer scs.wg.Done()

	// æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰éœ€è¦çˆ¬å–çš„åˆ›ä½œè€…
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-scs.stopChan:
			log.Println("ğŸ“ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡ºè°ƒåº¦å¾ªç¯")
			return
		case <-ticker.C:
			scs.performScheduledCrawl()
		}
	}
}

// performScheduledCrawl æ‰§è¡Œå®šæ—¶çˆ¬å–
func (scs *ScheduledCrawlerService) performScheduledCrawl() {
	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	// æŸ¥æ‰¾éœ€è¦çˆ¬å–çš„åˆ›ä½œè€…
	creatorsToProcess, err := scs.getCreatorsReadyForCrawl(ctx)
	if err != nil {
		log.Printf("âŒ è·å–å¾…çˆ¬å–åˆ›ä½œè€…å¤±è´¥: %v", err)
		return
	}

	if len(creatorsToProcess) == 0 {
		log.Println("ğŸ“‹ å½“å‰æ²¡æœ‰éœ€è¦çˆ¬å–çš„åˆ›ä½œè€…")
		return
	}

	log.Printf("ğŸ¯ æ‰¾åˆ° %d ä¸ªåˆ›ä½œè€…éœ€è¦çˆ¬å–", len(creatorsToProcess))

	// å¹¶å‘å¤„ç†æ¯ä¸ªåˆ›ä½œè€…ï¼ˆé™åˆ¶å¹¶å‘æ•°ï¼‰
	semaphore := make(chan struct{}, 3) // æœ€å¤š3ä¸ªå¹¶å‘çˆ¬å–ä»»åŠ¡
	var wg sync.WaitGroup

	for _, creator := range creatorsToProcess {
		wg.Add(1)
		go func(c models.Creator) {
			defer wg.Done()
			semaphore <- struct{}{}        // è·å–ä¿¡å·é‡
			defer func() { <-semaphore }() // é‡Šæ”¾ä¿¡å·é‡

			scs.crawlCreatorContent(c)
		}(creator)
	}

	wg.Wait()
	log.Println("âœ… æœ¬è½®çˆ¬å–ä»»åŠ¡å®Œæˆ")
}

// getCreatorsReadyForCrawl è·å–å‡†å¤‡çˆ¬å–çš„åˆ›ä½œè€…
func (scs *ScheduledCrawlerService) getCreatorsReadyForCrawl(ctx context.Context) ([]models.Creator, error) {
	now := time.Now()

	// æŸ¥è¯¢æ¡ä»¶ï¼šå¯ç”¨è‡ªåŠ¨çˆ¬å– ä¸” (ä¸‹æ¬¡çˆ¬å–æ—¶é—´å·²åˆ° æˆ– é¦–æ¬¡çˆ¬å–)
	filter := bson.M{
		"auto_crawl_enabled": true,
		"crawl_status":       bson.M{"$ne": "crawling"}, // ä¸æ˜¯æ­£åœ¨çˆ¬å–çŠ¶æ€
		"$or": []bson.M{
			{"next_crawl_at": bson.M{"$lte": now}},      // ä¸‹æ¬¡çˆ¬å–æ—¶é—´å·²åˆ°
			{"next_crawl_at": bson.M{"$exists": false}}, // é¦–æ¬¡çˆ¬å–
		},
	}

	cursor, err := scs.db.Collection("creators").Find(ctx, filter)
	if err != nil {
		return nil, err
	}
	defer cursor.Close(ctx)

	var creators []models.Creator
	if err := cursor.All(ctx, &creators); err != nil {
		return nil, err
	}

	return creators, nil
}

// crawlCreatorContent çˆ¬å–æŒ‡å®šåˆ›ä½œè€…çš„å†…å®¹
func (scs *ScheduledCrawlerService) crawlCreatorContent(creator models.Creator) {
	log.Printf("ğŸ•·ï¸ å¼€å§‹çˆ¬å–åˆ›ä½œè€…: %s (%s)", creator.DisplayName, creator.Platform)

	// æ›´æ–°çˆ¬å–çŠ¶æ€
	scs.updateCreatorCrawlStatus(creator.ID, "crawling", "")

	// å‡†å¤‡çˆ¬å–è¯·æ±‚
	crawlReq := CrawlRequest{
		Platform:   creator.Platform,
		CreatorURL: creator.ProfileURL,
		Limit:      20, // æ¯æ¬¡æœ€å¤šçˆ¬å–20æ¡
	}

	// è°ƒç”¨Pythonçˆ¬è™«æœåŠ¡
	posts, err := scs.callPythonCrawler(crawlReq)
	if err != nil {
		log.Printf("âŒ çˆ¬å– %s å¤±è´¥: %v", creator.DisplayName, err)
		scs.updateCreatorCrawlStatus(creator.ID, "failed", err.Error())
		return
	}

	// ä¿å­˜çˆ¬å–ç»“æœï¼ˆå¢é‡æ›´æ–°ï¼‰
	savedCount, err := scs.saveIncrementalPosts(creator.ID, posts)
	if err != nil {
		log.Printf("âŒ ä¿å­˜ %s çš„å†…å®¹å¤±è´¥: %v", creator.DisplayName, err)
		scs.updateCreatorCrawlStatus(creator.ID, "failed", err.Error())
		return
	}

	// æ›´æ–°çˆ¬å–çŠ¶æ€å’Œæ—¶é—´
	now := time.Now()
	nextCrawl := now.Add(time.Duration(creator.CrawlInterval) * time.Minute)

	scs.updateCreatorAfterCrawl(creator.ID, now, nextCrawl, savedCount)

	log.Printf("âœ… å®Œæˆçˆ¬å– %s: æ–°å¢ %d æ¡å†…å®¹", creator.DisplayName, savedCount)
}

// callPythonCrawler è°ƒç”¨Pythonçˆ¬è™«æœåŠ¡
func (scs *ScheduledCrawlerService) callPythonCrawler(req CrawlRequest) ([]PostData, error) {
	reqBody, err := json.Marshal(req)
	if err != nil {
		return nil, fmt.Errorf("åºåˆ—åŒ–è¯·æ±‚å¤±è´¥: %v", err)
	}

	resp, err := http.Post(PYTHON_CRAWLER_URL+"/crawl", "application/json", bytes.NewBuffer(reqBody))
	if err != nil {
		return nil, fmt.Errorf("è°ƒç”¨Pythonçˆ¬è™«æœåŠ¡å¤±è´¥: %v", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("Pythonçˆ¬è™«æœåŠ¡è¿”å›é”™è¯¯: %d - %s", resp.StatusCode, string(body))
	}

	var crawlResp CrawlResponse
	if err := json.NewDecoder(resp.Body).Decode(&crawlResp); err != nil {
		return nil, fmt.Errorf("è§£æçˆ¬è™«å“åº”å¤±è´¥: %v", err)
	}

	return crawlResp.Posts, nil
}

// saveIncrementalPosts å¢é‡ä¿å­˜å¸–å­ï¼ˆé¿å…é‡å¤ï¼‰
func (scs *ScheduledCrawlerService) saveIncrementalPosts(creatorID primitive.ObjectID, posts []PostData) (int, error) {
	if len(posts) == 0 {
		return 0, nil
	}

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	savedCount := 0
	collection := scs.db.Collection("posts")

	for _, post := range posts {
		// ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡
		contentHash := scs.generateContentHash(post.Title + "|" + post.Content)

		// æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
		filter := bson.M{
			"$or": []bson.M{
				{"content_hash": contentHash},
				{"$and": []bson.M{
					{"creator_id": creatorID},
					{"origin_id": post.OriginID},
					{"origin_id": bson.M{"$ne": ""}},
				}},
			},
		}

		count, err := collection.CountDocuments(ctx, filter)
		if err != nil {
			log.Printf("æ£€æŸ¥é‡å¤å†…å®¹å¤±è´¥: %v", err)
			continue
		}

		if count > 0 {
			continue // è·³è¿‡é‡å¤å†…å®¹
		}

		// åˆ›å»ºæ–°å¸–å­
		newPost := models.Post{
			ID:        primitive.NewObjectID(),
			CreatorID: creatorID,
			Platform:  post.Platform,
			PostID:    post.OriginID,
			Content:   post.Title + "\n" + post.Content,
			MediaURLs: append(post.Images, post.VideoURL),
			CreatedAt: time.Now(),
		}

		_, err = collection.InsertOne(ctx, newPost)
		if err != nil {
			log.Printf("ä¿å­˜å¸–å­å¤±è´¥: %v", err)
			continue
		}

		savedCount++
	}

	return savedCount, nil
}

// updateCreatorCrawlStatus æ›´æ–°åˆ›ä½œè€…çˆ¬å–çŠ¶æ€
func (scs *ScheduledCrawlerService) updateCreatorCrawlStatus(creatorID primitive.ObjectID, status, errorMsg string) {
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	update := bson.M{
		"$set": bson.M{
			"crawl_status": status,
			"updated_at":   time.Now(),
		},
	}

	if errorMsg != "" {
		update["$set"].(bson.M)["crawl_error"] = errorMsg
	} else {
		update["$unset"] = bson.M{"crawl_error": ""}
	}

	scs.db.Collection("creators").UpdateOne(ctx, bson.M{"_id": creatorID}, update)
}

// updateCreatorAfterCrawl çˆ¬å–å®Œæˆåæ›´æ–°åˆ›ä½œè€…ä¿¡æ¯
func (scs *ScheduledCrawlerService) updateCreatorAfterCrawl(creatorID primitive.ObjectID, lastCrawl, nextCrawl time.Time, savedCount int) {
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	update := bson.M{
		"$set": bson.M{
			"crawl_status":  "idle",
			"last_crawl_at": lastCrawl,
			"next_crawl_at": nextCrawl,
			"updated_at":    time.Now(),
		},
		"$unset": bson.M{"crawl_error": ""},
	}

	scs.db.Collection("creators").UpdateOne(ctx, bson.M{"_id": creatorID}, update)
}

// generateContentHash ç”Ÿæˆå†…å®¹å“ˆå¸Œ
func (scs *ScheduledCrawlerService) generateContentHash(content string) string {
	// è¿™é‡Œå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„å“ˆå¸Œç®—æ³•
	// æš‚æ—¶ä½¿ç”¨ç®€å•çš„é•¿åº¦+å‰åå­—ç¬¦ç»„åˆ
	if len(content) < 10 {
		return content
	}
	return fmt.Sprintf("%d_%s_%s", len(content), content[:5], content[len(content)-5:])
}
